<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script src="/urgent2026/assets/js/distillpub/template.v2.js"></script> <script src="/urgent2026/assets/js/distillpub/transforms.v2.js"></script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Data | URGENT Challenge </title> <meta name="author" content="URGENT Challenge"> <meta name="description" content="Universality, Robustness, and Generalizability for EnhancemeNT"> <meta name="keywords" content="speech enhancement"> <link rel="stylesheet" href="/urgent2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/urgent2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/urgent2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/urgent2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/urgent2026/assets/img/logo.jpg?6cb554ce913190e9422d73779c840cc3"> <link rel="stylesheet" href="/urgent2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://urgent-challenge.github.io/urgent2026/data/"> <script src="/urgent2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/urgent2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <li class="nav-item dropdown" style="list-style-type: none;"> <a class="navbar-brand title font-weight-lighter dropdown-toggle" href="#" id="yearDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">URGENT <span class="font-weight-bold">Challenge</span></a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="yearDropdown"> <a class="dropdown-item" href="https://urgent-challenge.github.io/urgent2024/">2024</a> <a class="dropdown-item" href="https://urgent-challenge.github.io/urgent2025/">2025</a> <a class="dropdown-item" href="/urgent2026/">2026</a> </div> </li> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/urgent2026/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/urgent2026/timeline/">Timeline </a> </li> <li class="nav-item active"> <a class="nav-link" href="/urgent2026/data/">Data <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/urgent2026/rules/">Rules </a> </li> <li class="nav-item "> <a class="nav-link" href="/urgent2026/baseline/">Baseline </a> </li> <li class="nav-item "> <a class="nav-link" href="/urgent2026/submission/">Submission </a> </li> <li class="nav-item "> <a class="nav-link" href="/urgent2026/leaderboard/">Leaderboard </a> </li> <li class="nav-item "> <a class="nav-link" href="/urgent2026/faq/">FAQ </a> </li> <li class="nav-item "> <a class="nav-link" href="/urgent2026/notice/">Notice </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">About Us </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/urgent2026/organizers/">Organizers</a> <a class="dropdown-item " href="/urgent2026/contact/">Contact</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Data</h1> <p class="post-description"></p> </header> <article> <h2 id="contents">Contents:</h2> <ul> <li><a href="#contents">Contents:</a></li> <li> <a href="#brief-data-description-track-1">Brief data description: Track 1</a> <ul> <li><a href="#trainingvalidation-data">Training/validation data</a></li> <li><a href="#non-blind-test-set">Non-blind test set</a></li> <li><a href="#blind-test-set">Blind test set</a></li> <li><a href="#detailed-data-description">Detailed data description</a></li> <li> <a href="#data-selection-and-simulation">Data selection and Simulation</a> <ul> <li><a href="#distortions">Distortions</a></li> <li><a href="#simulation-metadata">Simulation metadata</a></li> </ul> </li> </ul> </li> <li><a href="#data-description-track-2">Track 2</a></li> </ul> <h2 id="brief-data-description-track-1">Brief data description: Track 1</h2> <h4 id="trainingvalidation-data">Training/validation data</h4> <p>The training and validation data are both simulated by using several public speech/noise/rir corpora (see the table below for more details). We provide the data preparation pipeline with the <a href="https://github.com/urgent-challenge/urgent2026_challenge_track1" rel="external nofollow noopener" target="_blank">official baseline</a>, which automatically downloads and pre-processes the data.</p> <p>The <a href="https://github.com/urgent-challenge/urgent2026_challenge_track1/blob/main/utils/prepare_train_data.sh" rel="external nofollow noopener" target="_blank">data preparetion script</a> will generate two types of training data:</p> <p>The first is the pre-simulated data, which has the following form of directory structure:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>data/train_simulation/
├── speech_length.scp <span class="c"># Speech duration in number of sample points.</span>
├── spk1.scp <span class="c"># Clean speech file list of id and audio path.</span>
├── utt2fs  <span class="c"># id to sampling rate mapping</span>
├── utt2spk <span class="c"># utterance to speaker mapping </span>
└── wav.scp <span class="c"># Noisy speech file list of id and audio path.</span>
</code></pre></div></div> <p>The pre-simulated dataset can be loaded by the <code class="language-plaintext highlighter-rouge">PreSimulatedDataset</code> in the <a href="https://github.com/urgent-challenge/urgent2026_challenge_track1/blob/main/baseline_code/dataset.py" rel="external nofollow noopener" target="_blank">baseline code</a> .</p> <p>The other is the dynamic mixing dataset; we also provided a <code class="language-plaintext highlighter-rouge">DynamicMixingDataset</code> class in the <a href="https://github.com/urgent-challenge/urgent2026_challenge_track1/blob/main/baseline_code/dataset.py" rel="external nofollow noopener" target="_blank">baseline code</a> for loading data in a dynamic mixing manner. The dataset has the following form of directory structure:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>data/train_sources
├── noise_scoures.scp <span class="c"># Noise audio id and audio path.</span>
├── rirs.scp <span class="c"># Room impulse response id and audio path.</span>
├── source_length.scp <span class="c"># Speech duration in number of sample points.</span>
├── speech_sources.scp <span class="c"># Clean speech id and audio path.</span>
└── wind_noise_scoures.scp <span class="c"># # Wind noise audio id and audio path.</span>
</code></pre></div></div> <p>By default, we only generate the pre-simulated data for validation.</p> <h4 id="non-blind-test-set">Non-blind test set</h4> <ul> <li>Available online after November 3rd, 2025.</li> </ul> <h4 id="blind-test-set">Blind test set</h4> <ul> <li>Available online after November 18th, 2025.</li> </ul> <p><br></p> <h3 id="detailed-data-description">Detailed data description</h3> <p>The training and validation data are both simulated based on the following source data. Based on the <a href="https://urgent-challenge.github.io/urgent2025/data/">dataset of the 2nd URGENT challenge</a>, we conducted a data selection using the data filtering method proposed in a recent paper <d-cite key="liLessMoreData2025"></d-cite>.</p> <p><strong>It is noted that we encourage you to explore better ways of data selection and utilization in this challenge.</strong> In addition to the data and filtering methods provided by our baseline, you can make use of larger-scale datasets, such as the <a href="https://urgent-challenge.github.io/urgent2025/data/">track2 data</a> from the 2nd URGENT challenge, or other allowed data (please check it in the <a href="/urgent2026/rules/">rules pages</a>).</p> <style>table{border-spacing:0;border-collapse:collapse;width:100%;max-width:100%;margin-bottom:15px;background-color:transparent;text-align:left}th{font-weight:bold;border:1px solid #ccc;padding:8px}td{border:1px solid #ccc;padding:8px}tr{background-color:white}tr:nth-of-type(2n){background-color:#f6f8fa}.light-keys tr:nth-of-type(2n){background-color:black}.light-keys tr:hover{background-color:black}.light-keys table{border:0}.light-keys tr{border:0}.light-keys td{border:0}.light-keys th{border:0}tr th{background-color:white}tr{-moz-transition:background-color 300ms ease-in-out 0;-ms-transition:background-color 300ms ease-in-out 0;-o-transition:background-color 300ms ease-in-out 0;-webkit-transition:background-color 300ms ease-in-out 0;transition:background-color 300ms ease-in-out 0}tr:hover{background-color:#fff176}tr{border-top:1px solid #ccc;border-bottom:1px solid #ccc}</style> <table> <colgroup> <col> <col> <col> <col> <col> <col> </colgroup> <thead> <tr> <th>Type</th> <th>Corpus</th> <th>Condition</th> <th>Sampling Frequency (kHz)</th> <th>Duration of in 2nd URGENT</th> <th>Duration of in 3rd URGENT</th> <th>License</th> </tr> </thead> <tbody> <tr> <td rowspan="10">Speech</td> <td>LibriVox data from <a href="https://github.com/microsoft/DNS-Challenge/blob/master/download-dns-challenge-5-headset-training.sh" rel="external nofollow noopener" target="_blank">DNS5 challenge</a> </td> <td>Audiobook</td> <td>8~48</td> <td>~350 h</td> <td>~150 h</td> <td>CC BY 4.0</td> </tr> <tr> <td>LibriTTS reading speech</td> <td>Audiobook</td> <td>8~24</td> <td>~200 h</td> <td>~109 h</td> <td>CC BY 4.0</td> </tr> <tr> <td>VCTK reading speech</td> <td>Newspaper, etc.</td> <td>48</td> <td>~80 h</td> <td>~44 h</td> <td>ODC-BY</td> </tr> <tr> <td>EARS speech</td> <td>Studio recording</td> <td>48</td> <td>~100 h</td> <td>~16 h</td> <td> CC-NC 4.0</td> </tr> <tr> <td>Multilingual Librispeech (de, en, es, fr)<d-footnote>We collected less compressed MLS from LibriVox, which have higher audio quality than the original MLS for ASR.</d-footnote> </td> <td>Audiobook</td> <td>8~48</td> <td>~450 (48600) h</td> <td>~129 h</td> <td>CC0</td> </tr> <tr> <td>CommonVoice 19.0 (de, en, es, fr, zh-CN)</td> <td>Crowd-sourced voices</td> <td>8~48</td> <td>~1300 (9500) h</td> <td>~250 h</td> <td>CC0</td> </tr> <tr> <td>NNCES</td> <td>Children speech</td> <td>44.1</td> <td>-</td> <td>~20 h</td> <td>CC0</td> </tr> <tr> <td>SeniorTalk</td> <td>Elderly speech</td> <td>16</td> <td>-</td> <td>~50 h</td> <td>CC BY-NC-SA 4.0</td> </tr> <tr> <td>VocalSet</td> <td>Singing voice</td> <td>44.1</td> <td>-</td> <td>~10 h</td> <td>CC BY 4.0</td> </tr> <tr> <td>ESD</td> <td>Emotional speech</td> <td>16</td> <td>-</td> <td>~30 h</td> <td>non-commercial, <a href="https://github.com/HLTSingapore/Emotional-Speech-Data" rel="external nofollow noopener" target="_blank">custom</a> <d-footnote>You need to sign a license to obtain this dataset.</d-footnote> </td> </tr> </tbody> </table> <p>For the noise source and RIRs, we follow the same configuration as in the <a href="https://urgent-challenge.github.io/urgent2025/data/">2nd URGENT challenge</a>.</p> <table> <colgroup> <col> <col> <col> <col> <col> <col> </colgroup> <thead> <tr> <th>Type</th> <th>Corpus</th> <th>Condition</th> <th>Sampling Frequency (kHz)</th> <th>Duration of in 2nd URGENT</th> <th>License</th> </tr> </thead> <tbody> <tr> <td rowspan="5">Noise</td> <td>Audioset+FreeSound noise in DNS5 challenge</td> <td>Crowd-sourced + Youtube</td> <td>8~48</td> <td>~180 h</td> <td>CC BY 4.0</td> </tr> <tr> <td>WHAM! noise</td> <td>4 Urban environments</td> <td>48</td> <td>~70 h</td> <td>CC BY-NC 4.0</td> </tr> <tr> <td>FSD50K (human voice filtered)</td> <td>Crowd-sourced</td> <td>8~48</td> <td>~100 h</td> <td>CC0, CC-BY, CC-BY-NC, CC Sampling+</td> </tr> <tr> <td>Free Music Archive (medium)</td> <td>Free Music Archive (directed by WFMU) </td> <td>8~44.1</td> <td>~200 h</td> <td>CC</td> </tr> <tr> <td>Wind noise simulated by participants</td> <td>-</td> <td>any</td> <td>-</td> <td>-</td> </tr> <tr> <td rowspan="2">RIR</td> <td>Simulated RIRs from DNS5 challenge</td> <td><a href="https://www.openslr.org/28/" rel="external nofollow noopener" target="_blank">SLR28</a></td> <td>48</td> <td>~60k samples</td> <td>CC BY 4.0</td> </tr> <tr> <td>RIRs simulated by participants</td> <td>-</td> <td>any&lt;/a&gt;</td> <td>-</td> <td>-</td> </tr> </tbody> </table> <blockquote> <p>We allow participants to simulate their own RIRs using existing tools<d-footnote>For example, <a href="https://github.com/ehabets/RIR-Generator" rel="external nofollow noopener" target="_blank">RIR-Generator</a>, <a href="https://github.com/LCAV/pyroomacoustics" rel="external nofollow noopener" target="_blank">pyroomacoustics</a>, <a href="https://github.com/DavidDiazGuerra/gpuRIR" rel="external nofollow noopener" target="_blank">gpuRIR</a>, and so on.</d-footnote> for generating the training data. The participants can also propose publicly available real recorded RIRs to be included in the above data list during the grace period (See <a href="/urgent2026/timeline"><code class="language-plaintext highlighter-rouge">Timeline</code></a>). Note: If participants used additional RIRs to train their model, the related information should be provided in the README.yaml file in the submission. Check the <a href="/urgent2026/template">template</a> for more information.</p> <p>We allow participants to simulate wind noise using some tools such as <a href="https://github.com/audiolabs/SC-Wind-Noise-Generator/tree/main" rel="external nofollow noopener" target="_blank">SC-Wind-Noise-Generator</a>. In default, the simulation script in our repository simulates 200 and 100 wind noises for training and validation for each sampling frequency. The configuration can be easily changed in <a href="https://github.com/urgent-challenge/urgent2026_challenge_track1/blob/main/conf/wind_noise_simulation_train.yaml" rel="external nofollow noopener" target="_blank">wind_noise_simulation_train.yaml</a> and <a href="https://github.com/urgent-challenge/urgent2026_challenge_track1/blob/main/conf/wind_noise_simulation_validation.yaml" rel="external nofollow noopener" target="_blank">wind_noise_simulation_validation.yaml</a></p> </blockquote> <p><br></p> <h3 id="data-selection-and-simulation">Data selection and Simulation</h3> <p>We apply the data selection to the Track 1 data of the 2nd URGENT using the data filtering method proposed in the recent paper <d-cite key="liLessMoreData2025"></d-cite>. The selected data list is available at <a href="https://github.com/urgent-challenge/urgent2026_challenge_track1/blob/main/meta/train_selected_700h" rel="external nofollow noopener" target="_blank">here</a>. The speech source from NNCES, SeniorTalk, VocalSet, and ESD is not filtered.</p> <p>Note that the data filtering of paper <d-cite key="liLessMoreData2025"></d-cite> is not the best method to utilize the large-scale dataset for SE. <strong>The goal of this challenge is to encourage participants to develop how to better leverage large-scale data</strong> to improve the final SE performance.</p> <p>The simulation data can be generated as follows:</p> <ol> <li> <p>In the first step, a manifest <code class="language-plaintext highlighter-rouge">meta.tsv</code> is first generated by <a href="https://github.com/urgent-challenge/urgent2025_challenge/blob/main/simulation/generate_data_param.py" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">simulation/generate_data_param.py</code></a> from the given list of speech, noise, and room impulse response (RIR) samples. It specifies how each sample will be simulated, including the type of distortion to be applied, the speech/noise/RIR sample to be used, the signal-to-noise ratio (SNR), the random seed, and so on.</p> </li> <li> <p>In the second step, the simulation can be done in parallel via <a href="https://github.com/urgent-challenge/urgent2025_challenge/blob/main/simulation/simulate_data_from_param.py" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">simulation/simulate_data_from_param.py</code></a> for different samples according to the manifest while ensuring reproducibility. This procedure can be used to generate training and validation datasets.</p> </li> <li> <p>By default, we applied a <a href="https://github.com/urgent-challenge/urgent2026_challenge_track1/blob/main/simulation/simulate_data_from_param.py" rel="external nofollow noopener" target="_blank">high-pass filter</a> to the speech signals since we have noticed that there is high-energy noise in the infrasound frequency band in some speech sources. You can turn it off by setting <code class="language-plaintext highlighter-rouge">highpass=False</code> in your simulation.</p> </li> </ol> <p>For the training set, <strong>we recommend dynamically generating degraded speech samples during training</strong> to increase the data diversity.</p> <h4 id="distortions">Distortions</h4> <p>In this challenge, the SE system has to address the following seven distortions. In addition to the first four distortions considered in our first challenge, <strong>we added three more distortions (bold ones) often observed in real recordings</strong>. Furthermore, in this challenge, <strong>inputs may have multiple distortions</strong>.</p> <p>We provide an example simulation script as <a href="https://github.com/urgent-challenge/urgent2026_challenge_track1/blob/main/simulation/simulate_data_from_param.py" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">simulation/simulate_data_from_param.py</code></a>.</p> <ol> <li>Additive noise</li> <li>Reverberation</li> <li>Clipping</li> <li>Bandwidth limitation</li> <li><strong>Codec distortion</strong></li> <li><strong>Packet loss</strong></li> <li><strong>Wind noise</strong></li> </ol> <h4 id="simulation-metadata">Simulation metadata</h4> <p>The manifest mentioned above is a <code class="language-plaintext highlighter-rouge">tsv</code> file containing several columns (separated by <code class="language-plaintext highlighter-rouge">\t</code>). For example:</p> <table> <thead> <tr> <th style="text-align: center">id</th> <th style="text-align: center">noisy_path</th> <th style="text-align: center">speech_uid</th> <th style="text-align: center">speech_sid</th> <th style="text-align: center">clean_path</th> <th style="text-align: center">noise_uid</th> <th style="text-align: center">snr_dB</th> <th style="text-align: center">rir_uid</th> <th style="text-align: center">augmentation</th> <th style="text-align: center">fs</th> <th style="text-align: center">length</th> <th>text</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">unique ID</td> <td style="text-align: center">path to the generated degraded speech</td> <td style="text-align: center">utterance ID of clean speech</td> <td style="text-align: center">speaker ID of clean speech</td> <td style="text-align: center">path to the paired clean speech</td> <td style="text-align: center">utterance ID of noise</td> <td style="text-align: center">SNR in decibel</td> <td style="text-align: center">utterance ID of the RIR</td> <td style="text-align: center">augmentation type</td> <td style="text-align: center">sampling frequency</td> <td style="text-align: center">sample length</td> <td>raw transcript</td> </tr> <tr> <td style="text-align: center">fileid_1</td> <td style="text-align: center">simulation_validation/noisy/fileid_1.flac</td> <td style="text-align: center">p226_001_mic1</td> <td style="text-align: center">vctk_p226</td> <td style="text-align: center">simulation_validation/clean/fileid_1.flac</td> <td style="text-align: center">JC1gBY5vXHI</td> <td style="text-align: center">16.106643714525433</td> <td style="text-align: center">mediumroom-Room119-00056</td> <td style="text-align: center">bandwidth_limitation-kaiser_fast-&gt;24000</td> <td style="text-align: center">48000</td> <td style="text-align: center">134338</td> <td>Please call Stella.</td> </tr> <tr> <td style="text-align: center">fileid_2</td> <td style="text-align: center">simulation_validation/noisy/fileid_2.flac</td> <td style="text-align: center">p226_001_mic2</td> <td style="text-align: center">vctk_p226</td> <td style="text-align: center">simulation_validation/clean/fileid_2.flac</td> <td style="text-align: center">file205_039840000_loc32_day1</td> <td style="text-align: center">2.438365163611807</td> <td style="text-align: center">none</td> <td style="text-align: center">bandwidth_limitation-kaiser_best-&gt;22050</td> <td style="text-align: center">48000</td> <td style="text-align: center">134338</td> <td>Please call Stella.</td> </tr> <tr> <td style="text-align: center">fileid_1561</td> <td style="text-align: center">simulation_validation/noisy/fileid_1561.flac</td> <td style="text-align: center">p315_001_mic1</td> <td style="text-align: center">vctk_p315</td> <td style="text-align: center">simulation_validation/clean/fileid_1561.flac</td> <td style="text-align: center">AvbnjyrHq8M</td> <td style="text-align: center">1.3502745341597029</td> <td style="text-align: center">mediumroom-Room076-00093</td> <td style="text-align: center">clipping(min=0.016037324066971528,<wbr></wbr>max=0.9890219800761639)</td> <td style="text-align: center">48000</td> <td style="text-align: center">114829</td> <td>&lt;not-available&gt;</td> </tr> </tbody> </table> <p><strong>Note</strong></p> <ul> <li> <p>If the <code class="language-plaintext highlighter-rouge">rir_uid</code> value is not <code class="language-plaintext highlighter-rouge">none</code>, the specified RIR is applied to the clean speech sample.</p> </li> <li> <p>If the <code class="language-plaintext highlighter-rouge">augmentation</code> value is not <code class="language-plaintext highlighter-rouge">none</code>, the specified augmentation is applied to the degraded speech sample.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">&lt;not-available&gt;</code> in the <code class="language-plaintext highlighter-rouge">text</code> column is a placeholder for transcripts that are not available.</p> </li> <li> <p>The audios in <code class="language-plaintext highlighter-rouge">noisy_path</code>, <code class="language-plaintext highlighter-rouge">clean_path</code>, and (optional) <code class="language-plaintext highlighter-rouge">noise_path</code> are consistently scaled such that <code class="language-plaintext highlighter-rouge">noisy_path = clean_path + noise_path</code>.</p> </li> <li> <p>However, the scale of the enhanced audio is not critical for objective evaluation the challenge, as the evaluation metrics are made largely insensitive to the scale. For subjective listening in the final phase, however, it is recommended that the participants properly scale the enhanced audios to facilitate a consistent evaluation.</p> </li> <li> <p>For all different distortion types, the original sampling frequency of each clean speech sample is always preserved, i.e., the degraded speech sample also shares the same sampling frequency. For <code class="language-plaintext highlighter-rouge">bandwidth_limitation</code> augmentation, this means that the generated speech sample is resampled to the original sampling frequency <code class="language-plaintext highlighter-rouge">fs</code>.</p> </li> </ul> <h1 id="data-description-track-2">Data description: Track 2</h1> <h2 id="trainingvalidation-data-1">Training/Validation Data</h2> <p>We allow the use of any public datasets for training. Please include the data usage in technical report. See https://github.com/urgent-challenge/urgent2026_challenge_track2 for training and validation data usage in official baseline.</p> <h2 id="non-blind-test-set-1">Non-blind test set</h2> <ul> <li>The urgent 2024 MOS dataset will be used for non-blind test sets. Available online after November 3rd, 2025</li> </ul> <h2 id="blind-test-set-1">Blind test set</h2> <ul> <li>Available online after November 18th, 2025</li> </ul> </article> </div> </div> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/urgent2026/assets/bibliography/data.bib"></d-bibliography> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 URGENT Challenge. Last updated: August 27, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/urgent2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/urgent2026/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/urgent2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/urgent2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/urgent2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/urgent2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/urgent2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/urgent2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/urgent2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/urgent2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>