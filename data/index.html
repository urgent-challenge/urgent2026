<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script src="/urgent2026/assets/js/distillpub/template.v2.js"></script> <script src="/urgent2026/assets/js/distillpub/transforms.v2.js"></script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Data | URGENT Challenge </title> <meta name="author" content="URGENT Challenge"> <meta name="description" content="Universality, Robustness, and Generalizability for EnhancemeNT"> <meta name="keywords" content="speech enhancement"> <link rel="stylesheet" href="/urgent2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/urgent2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/urgent2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/urgent2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/urgent2026/assets/img/logo.jpg?6cb554ce913190e9422d73779c840cc3"> <link rel="stylesheet" href="/urgent2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="127.0.0.1/urgent2026/data/"> <script src="/urgent2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/urgent2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <li class="nav-item dropdown" style="list-style-type: none;"> <a class="navbar-brand title font-weight-lighter dropdown-toggle" href="#" id="yearDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">URGENT <span class="font-weight-bold">Challenge</span></a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="yearDropdown"> <a class="dropdown-item" href="https://urgent-challenge.github.io/urgent2024/" rel="external nofollow noopener" target="_blank">2024</a> <a class="dropdown-item" href="https://urgent-challenge.github.io/urgent2025/" rel="external nofollow noopener" target="_blank">2025</a> <a class="dropdown-item" href="/urgent2026/">2026</a> </div> </li> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/urgent2026/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/urgent2026/timeline/">Timeline </a> </li> <li class="nav-item active"> <a class="nav-link" href="/urgent2026/data/">Data <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/urgent2026/rules/">Rules </a> </li> <li class="nav-item "> <a class="nav-link" href="/urgent2026/baseline/">Baseline </a> </li> <li class="nav-item "> <a class="nav-link" href="/urgent2026/submission/">Submission </a> </li> <li class="nav-item "> <a class="nav-link" href="/urgent2026/leaderboard/">Leaderboard </a> </li> <li class="nav-item "> <a class="nav-link" href="/urgent2026/faq/">FAQ </a> </li> <li class="nav-item "> <a class="nav-link" href="/urgent2026/notice/">Notice </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">About Us </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/urgent2026/organizers/">Organizers</a> <a class="dropdown-item " href="/urgent2026/contact/">Contact</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Data</h1> <p class="post-description"></p> </header> <article> <h2 id="brief-data-description">Brief data description</h2> <h4 id="trainingvalidation-data">Training/validation data</h4> <p>The training and validation data are both simulated by using several public speech/noise/rir corpora (see the table below for more details). We provide the <a href="https://github.com/urgent-challenge/urgent2025_challenge" rel="external nofollow noopener" target="_blank">data preparation script</a> which automatically downloads and pre-processes those data.</p> <p>There are two types of validation data. One is automatically generated during data preparation and the other is provided by the orgnizeres</p> <ul> <li>Unofficial validation set: participants can generate their own validation set using the validation subset of the official challenge datasets to choose the best model.</li> <li>Official validation set: we organizeres provide the validation set for the leaderboard submission. It contains 1000 samples and all of them are synthetic data. The maximum duration per utterance is 15 seconds. Unlike the unofficial one, <strong>we manually picked up clean speeches for the noisy corpora (i.e., CommonVoice)</strong> so it better reflectes the enhancement quality. <a href="https://drive.google.com/file/d/1Ip-C5tUNGCssT8KAjHUUoh99jkzRH6nm/view" rel="external nofollow noopener" target="_blank">Noisy speech</a>, <a href="https://drive.google.com/file/d/11geBBf24WKN1xT_NasnI4JrmKpqNo8h9/view" rel="external nofollow noopener" target="_blank">Clean speech</a>, and <a href="https://drive.google.com/file/d/1CU5QKYOgG4fUuJ8oAC6BEhI9ZDhQYZpF/view" rel="external nofollow noopener" target="_blank">metadata</a> are available.</li> </ul> <h4 id="non-blind-test-set">Non-blind test set</h4> <p>The non-blind test set are prepared in a similar way as the official validation set but there are several differences:</p> <ul> <li>The test split of the official challenge datasets were used.</li> <li>Several unseen noise and rir were also used when simulating the non-blind test set.</li> <li>Sampling rates are almost equally distributed (there are ~1000/7 data for each of 8k, 16k, 22.05k, 24k, 32k, 44.1k, and 48kHz). We downsampled some data to achieve this.</li> </ul> <p>The <a href="https://drive.google.com/file/d/1rxV6RgA4LAp2I1EnHsln7wI7-UCP6Qer/view" rel="external nofollow noopener" target="_blank">noisy</a> and <a href="https://drive.google.com/file/d/1RarjxOgWkaDV8EjH_eLX169y89PVa3sg/view?usp=sharing" rel="external nofollow noopener" target="_blank">clean</a> speeches as well as the <a href="https://drive.google.com/file/d/1CfhKjfkkUZ60UEOHnlntcQY2m_9pn1uA/view?usp=sharing" rel="external nofollow noopener" target="_blank">metadata</a> are available. After the non-blind test phase ends, clean speech and metadata will be available.</p> <h4 id="blind-test-set">Blind test set</h4> <p>The blind test set, which will <strong>be used for the final ranking</strong>, is available <a href="https://drive.google.com/file/d/1dHvYEGHCf9rsB1q-Cd9rXOeQaa_vjG2u/view?usp=sharing" rel="external nofollow noopener" target="_blank">here</a>.</p> <p>To evaluate the universality, robustness, and generalizability of the submitted systems as outlined in the theme of this challenge, the blind test set is primarily from domains other than the training set:</p> <ul> <li>consist of <strong>50% real-recorded data</strong> (without audio ground truth) and 50% synthetic data,</li> <li>primarily be derived from <strong>from unseen corpora/datasets</strong>,</li> <li>include <strong>one unseen language</strong> in addition to five languages present in the training data,</li> <li>be distorted by <strong>some unseen distortions</strong> in addition to those used in training, and</li> <li>contain 150 samples for each language, amounting to 900 samples in total.</li> </ul> <p>Note that the evaluation procedure during the blind testing phase differs from that in the validation/non-blind test phase in the following ways:</p> <ul> <li>Two additional metrics (POLQA and MOS) will be included. (As previously announced, only the English subset will be used for the MOS evaluation due to the short evaluation period.)</li> <li>Only a subset of metrics (DNSMOS, NISQA, UTMOS, MOS, and CER if transcription is available) will be considered for the real-recorded data.</li> </ul> <p><br></p> <h2 id="detailed-data-description">Detailed data description</h2> <p>The training and validation data are both simulated based on the following source data. Note that the validation set made by the <a href="(https://github.com/urgent-challenge/urgent2025_challenge)">provided script</a> is different from the official validation set used in the leaderboard, although the data source and the type of distortions do not change.</p> <p>The challenge has two tracks:</p> <ul> <li> <strong>First track</strong>: We limit the duration of MLS and CommonVoice, resulting in ~2.5k hours of speech.</li> <li> <strong>Second track</strong>: We do not limit the duration of MLS and CommonVoice datasets, resulting in ~60k hours of speech.</li> </ul> <style>table{border-spacing:0;border-collapse:collapse;width:100%;max-width:100%;margin-bottom:15px;background-color:transparent;text-align:left}th{font-weight:bold;border:1px solid #ccc;padding:8px}td{border:1px solid #ccc;padding:8px}tr{background-color:white}tr:nth-of-type(2n){background-color:#f6f8fa}.light-keys tr:nth-of-type(2n){background-color:black}.light-keys tr:hover{background-color:black}.light-keys table{border:0}.light-keys tr{border:0}.light-keys td{border:0}.light-keys th{border:0}tr th{background-color:white}tr{-moz-transition:background-color 300ms ease-in-out 0;-ms-transition:background-color 300ms ease-in-out 0;-o-transition:background-color 300ms ease-in-out 0;-webkit-transition:background-color 300ms ease-in-out 0;transition:background-color 300ms ease-in-out 0}tr:hover{background-color:#fff176}tr{border-top:1px solid #ccc;border-bottom:1px solid #ccc}</style> <table> <colgroup> <col> <col> <col> <col> <col> <col> </colgroup> <thead> <tr> <th>Type</th> <th>Corpus</th> <th>Condition</th> <th>Sampling Frequency (kHz)</th> <th>Duration of track 1 (track2)</th> <th>License</th> </tr> </thead> <tbody> <tr> <td rowspan="7">Speech</td> <td>LibriVox data from <a href="https://github.com/microsoft/DNS-Challenge/blob/master/download-dns-challenge-5-headset-training.sh" rel="external nofollow noopener" target="_blank">DNS5 challenge</a> </td> <td>Audiobook</td> <td>8~48</td> <td>~350 h</td> <td>CC BY 4.0</td> </tr> <tr> <td>LibriTTS reading speech</td> <td>Audiobook</td> <td>8~24</td> <td>~200 h</td> <td>CC BY 4.0</td> </tr> <tr> <td>VCTK reading speech</td> <td>Newspaper, etc.</td> <td>48</td> <td>~80 h</td> <td>ODC-BY</td> </tr> <tr> <td>WSJ reading speech</td> <td>WSJ news</td> <td>16</td> <td>~85 h</td> <td>LDC User Agreement</td> </tr> <tr> <td>EARS speech</td> <td>Studio recording</td> <td>48</td> <td>~100 h</td> <td> CC-NC 4.0</td> </tr> <tr> <td>Multilingual Librispeech (de, en, es, fr)<d-footnote>We collected less compressed MLS from LibriVox, which have higher audio quality than the original MLS for ASR.</d-footnote> </td> <td>Audiobook</td> <td>8~48</td> <td>~450 (48600) h</td> <td>CC0</td> </tr> <tr> <td>CommonVoice 19.0 (de, en, es, fr, zh-CN)</td> <td>Crowd-sourced voices</td> <td>8~48</td> <td>~1300 (9500) h</td> <td>CC0</td> </tr> <tr> <td rowspan="5">Noise</td> <td>Audioset+FreeSound noise in DNS5 challenge</td> <td>Crowd-sourced + Youtube</td> <td>8~48</td> <td>~180 h</td> <td>CC BY 4.0</td> </tr> <tr> <td>WHAM! noise</td> <td>4 Urban environments</td> <td>48</td> <td>~70 h</td> <td>CC BY-NC 4.0</td> </tr> <tr> <td>FSD50K (human voice filtered)</td> <td>Crowd-sourced</td> <td>8~48</td> <td>~100 h</td> <td>CC0, CC-BY, CC-BY-NC, CC Sampling+</td> </tr> <tr> <td>Free Music Archive (medium)</td> <td>Free Music Archive (directed by WFMU) </td> <td>8~44.1</td> <td>~200 h</td> <td>CC</td> </tr> <tr> <td>Wind noise simulated by participants</td> <td>-</td> <td>any</td> <td>-</td> <td>-</td> </tr> <tr> <td rowspan="2">RIR</td> <td>Simulated RIRs from DNS5 challenge</td> <td><a href="https://www.openslr.org/28/" rel="external nofollow noopener" target="_blank">SLR28</a></td> <td>48</td> <td>~60k samples</td> <td>CC BY 4.0</td> </tr> <tr> <td>RIRs simulated by participants</td> <td>-</td> <td>any&lt;/a&gt;</td> <td>-</td> <td>-</td> </tr> </tbody> </table> <blockquote> <p>For participants who need access to the WSJ data, please reach out to the organizers (<a href="mailto:urgent.challenge@gmail.com">urgent.challenge@gmail.com</a>) for a temporary license supported by LDC. Please include your name, organization/affiliation, and the username used in the leaderboard in the email for a smooth procedure. Note that we do not accept the request unless you have registered to the <a href="/urgent2025/leaderboard">leaderboard</a>.</p> <p>We allow participants to simulate their own RIRs using existing tools<d-footnote>For example, <a href="https://github.com/ehabets/RIR-Generator" rel="external nofollow noopener" target="_blank">RIR-Generator</a>, <a href="https://github.com/LCAV/pyroomacoustics" rel="external nofollow noopener" target="_blank">pyroomacoustics</a>, <a href="https://github.com/DavidDiazGuerra/gpuRIR" rel="external nofollow noopener" target="_blank">gpuRIR</a>, and so on.</d-footnote> for generating the training data. The participants can also propose publicly available real recorded RIRs to be included in the above data list during the grace period (See <a href="/urgent2025/timeline"><code class="language-plaintext highlighter-rouge">Timeline</code></a>). Note: If participants used additional RIRs to train their model, the related information should be provided in the README.yaml file in the submission. Check the <a href="/urgent2025/template">template</a> for more information.</p> <p>We allow participants to simulate wind noise using some tools such as <a href="https://github.com/audiolabs/SC-Wind-Noise-Generator/tree/main" rel="external nofollow noopener" target="_blank">SC-Wind-Noise-Generator</a>. In default, the simulation script in our repository simulates 200 and 100 wind noises for training and validation for each sampling frequency. The configuration can be easily changed in <a href="https://github.com/urgent-challenge/urgent2025_challenge/blob/main/conf/wind_noise_simulation_train.yaml" rel="external nofollow noopener" target="_blank">wind_noise_simulation_train.yaml</a> and <a href="https://github.com/urgent-challenge/urgent2025_challenge/blob/main/conf/wind_noise_simulation_validation.yaml" rel="external nofollow noopener" target="_blank">wind_noise_simulation_validation.yaml</a></p> </blockquote> <p><br></p> <h2 id="pre-processing">Pre-processing</h2> <p><img alt="pre-processing" src="/urgent2025/assets/img/preprocessing.png" style="max-width: 100%;"></p> <p>Before simulation, some speech and noise data are pre-processed to filter out low-quality samples and to detect the true sampling frequency (SF). Specifically, we applied data filtering (described below) to LibriVox and CommonVoice (track1)<d-footnote>In Track2, participants are allowed to use the entire data from CommonVoice 19.0 and we thus do not do any data filtering.</d-footnote>.</p> <p>The pre-processing procedure includes:</p> <ol> <li>We first estimate the effective bandwidth of each speech and noise sample based on the energy thresholding algorithm proposed in <d-cite key="Hi_Fi-Bakhturina2021"></d-cite><d-footnote><a href="https://github.com/urgent-challenge/urgent2025_challenge/blob/main/utils/estimate_audio_bandwidth.py" rel="external nofollow noopener" target="_blank">https://github.com/urgent-challenge/urgent2025_challenge/blob/main/utils/estimate_audio_bandwidth.py</a></d-footnote>. This is critical for our proposed method to successfully handle data with different SFs. Then, we resample each speech and noise sample accordingly to the best matching SF, which is defined as the lowest SF among {8, 16, 22.05, 24, 32, 44.1, 48} kHz that can fully cover the estimated effective bandwidth.</li> <li>A voice activity detection (VAD) algorithm<d-footnote><a href="https://github.com/urgent-challenge/urgent2025_challenge/blob/main/utils/filter_via_vad.py" rel="external nofollow noopener" target="_blank">https://github.com/urgent-challenge/urgent2025_challenge/blob/main/utils/filter_via_vad.py</a></d-footnote> is further used to detect “bad” speech samples that are actually non-speech or mostly silence, which will be removed from the data.</li> <li>Finally, the non-intrusive DNSMOS scores (OVRL, SIG, BAK)<d-cite key="DNSMOS-Reddy2022"></d-cite><d-footnote><a href="https://github.com/microsoft/DNS-Challenge/blob/master/DNSMOS/dnsmos_local.py" rel="external nofollow noopener" target="_blank">https://github.com/microsoft/DNS-Challenge/blob/master/DNSMOS/dnsmos_local.py</a></d-footnote> are calculated for each remaining speech sample. This allows us to filter out noisy and low-quality speech samples via thresholding each score<d-footnote><a href="https://github.com/urgent-challenge/urgent2025_challenge/blob/main/utils/filter_via_dnsmos.py" rel="external nofollow noopener" target="_blank">https://github.com/urgent-challenge/urgent2025_challenge/blob/main/utils/filter_via_dnsmos.py</a></d-footnote>.</li> </ol> <p>We finally curated a list of speech sample (~2500 hours) and noise samples (~500 hours) for Track1 based on the above procedure that will be used for simulating the training and validation data in the challenge.</p> <p>Note that the data filtering is inperfect and the dataset still has non-ignorable amount of noisy samples. <strong>One of the goal of this challenge is to encourage participants to develop how to leverage (or filter out) such noisy data</strong> to improve the final SE performance.</p> <p><br></p> <h2 id="simulation">Simulation</h2> <p>With the proviced scripts in the next section, the simulation data can be generated offline in two steps.</p> <ol> <li> <p>In the first step, a manifest <code class="language-plaintext highlighter-rouge">meta.tsv</code> is firstly generated by <a href="https://github.com/urgent-challenge/urgent2025_challenge/blob/main/simulation/generate_data_param.py" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">simulation/generate_data_param.py</code></a> from the given list of speech, noise, and room impulse response (RIR) samples. It specifies how each sample will be simulated, including the type of distortion to be applied, the speech/noise/RIR sample to be used, the signal-to-noise ratio (SNR), the random seed, and so on.</p> </li> <li> <p>In the second step, the simulation can be done in parallel via <a href="https://github.com/urgent-challenge/urgent2025_challenge/blob/main/simulation/simulate_data_from_param.py" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">simulation/simulate_data_from_param.py</code></a> for different samples according to the manifest while ensuring reproducibility. This procedure can be used to generate training and validation datasets.</p> </li> </ol> <p>For the training set, <strong>we recommend dynamically generating degraded speech samples during training</strong> to increase the data diversity.</p> <h4 id="distortions">Distortions</h4> <p>In this challenge, the SE system has to address the following seven distortions. In addition to the first four distortions considered in our first challenge, <strong>we added three more distortions (bold ones) often observed in real recordings</strong>. Furthermore, in this challenge, <strong>inputs may have multiple distortions</strong>.</p> <p>We provide an example simulation script as <a href="https://github.com/urgent-challenge/urgent2025_challenge/blob/main/simulation/simulate_data_from_param.py" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">simulation/simulate_data_from_param.py</code></a>.</p> <ol> <li>Additive noise</li> <li>Reverberation</li> <li>Clipping</li> <li>Bandwidth limitation</li> <li><strong>Codec distortion</strong></li> <li><strong>Packet loss</strong></li> <li><strong>Wind noise</strong></li> </ol> <h4 id="simulation-metadata">Simulation metadata</h4> <p>The manifest mentioned above is a <code class="language-plaintext highlighter-rouge">tsv</code> file containing several columns (separated by <code class="language-plaintext highlighter-rouge">\t</code>). For example:</p> <table> <thead> <tr> <th style="text-align: center">id</th> <th style="text-align: center">noisy_path</th> <th style="text-align: center">speech_uid</th> <th style="text-align: center">speech_sid</th> <th style="text-align: center">clean_path</th> <th style="text-align: center">noise_uid</th> <th style="text-align: center">snr_dB</th> <th style="text-align: center">rir_uid</th> <th style="text-align: center">augmentation</th> <th style="text-align: center">fs</th> <th style="text-align: center">length</th> <th>text</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">unique ID</td> <td style="text-align: center">path to the generated degraded speech</td> <td style="text-align: center">utterance ID of clean speech</td> <td style="text-align: center">speaker ID of clean speech</td> <td style="text-align: center">path to the paired clean speech</td> <td style="text-align: center">utterance ID of noise</td> <td style="text-align: center">SNR in decibel</td> <td style="text-align: center">utterance ID of the RIR</td> <td style="text-align: center">augmentation type</td> <td style="text-align: center">sampling frequency</td> <td style="text-align: center">sample length</td> <td>raw transcript</td> </tr> <tr> <td style="text-align: center">fileid_1</td> <td style="text-align: center">simulation_validation/noisy/fileid_1.flac</td> <td style="text-align: center">p226_001_mic1</td> <td style="text-align: center">vctk_p226</td> <td style="text-align: center">simulation_validation/clean/fileid_1.flac</td> <td style="text-align: center">JC1gBY5vXHI</td> <td style="text-align: center">16.106643714525433</td> <td style="text-align: center">mediumroom-Room119-00056</td> <td style="text-align: center">bandwidth_limitation-kaiser_fast-&gt;24000</td> <td style="text-align: center">48000</td> <td style="text-align: center">134338</td> <td>Please call Stella.</td> </tr> <tr> <td style="text-align: center">fileid_2</td> <td style="text-align: center">simulation_validation/noisy/fileid_2.flac</td> <td style="text-align: center">p226_001_mic2</td> <td style="text-align: center">vctk_p226</td> <td style="text-align: center">simulation_validation/clean/fileid_2.flac</td> <td style="text-align: center">file205_039840000_loc32_day1</td> <td style="text-align: center">2.438365163611807</td> <td style="text-align: center">none</td> <td style="text-align: center">bandwidth_limitation-kaiser_best-&gt;22050</td> <td style="text-align: center">48000</td> <td style="text-align: center">134338</td> <td>Please call Stella.</td> </tr> <tr> <td style="text-align: center">fileid_1561</td> <td style="text-align: center">simulation_validation/noisy/fileid_1561.flac</td> <td style="text-align: center">p315_001_mic1</td> <td style="text-align: center">vctk_p315</td> <td style="text-align: center">simulation_validation/clean/fileid_1561.flac</td> <td style="text-align: center">AvbnjyrHq8M</td> <td style="text-align: center">1.3502745341597029</td> <td style="text-align: center">mediumroom-Room076-00093</td> <td style="text-align: center">clipping(min=0.016037324066971528,<wbr></wbr>max=0.9890219800761639)</td> <td style="text-align: center">48000</td> <td style="text-align: center">114829</td> <td>&lt;not-available&gt;</td> </tr> </tbody> </table> <p><strong>Note</strong></p> <ul> <li> <p>If the <code class="language-plaintext highlighter-rouge">rir_uid</code> value is not <code class="language-plaintext highlighter-rouge">none</code>, the specified RIR is applied to the clean speech sample.</p> </li> <li> <p>If the <code class="language-plaintext highlighter-rouge">augmentation</code> value is not <code class="language-plaintext highlighter-rouge">none</code>, the specified augmentation is applied to the degraded speech sample.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">&lt;not-available&gt;</code> in the <code class="language-plaintext highlighter-rouge">text</code> column is a placeholder for transcripts that are not available.</p> </li> <li> <p>The audios in <code class="language-plaintext highlighter-rouge">noisy_path</code>, <code class="language-plaintext highlighter-rouge">clean_path</code>, and (optional) <code class="language-plaintext highlighter-rouge">noise_path</code> are consistently scaled such that <code class="language-plaintext highlighter-rouge">noisy_path = clean_path + noise_path</code>.</p> </li> <li> <p>However, the scale of the enhanced audio is not critical for objective evaluation the challenge, as the evaluation metrics are made largely insensitive to the scale. For subjective listening in the final phase, however, it is recommended that the participants properly scale the enhanced audios to facilitate a consistent evaluation.</p> </li> <li> <p>For all different distortion types, the original sampling frequency of each clean speech sample is always preserved, i.e., the degraded speech sample also shares the same sampling frequency. For <code class="language-plaintext highlighter-rouge">bandwidth_limitation</code> augmentation, this means that the generated speech sample is resampled to the original sampling frequency <code class="language-plaintext highlighter-rouge">fs</code>.</p> </li> </ul> </article> </div> </div> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/urgent2026/assets/bibliography/data.bib"></d-bibliography> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 URGENT Challenge. Last updated: August 13, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/urgent2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/urgent2026/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/urgent2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/urgent2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/urgent2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/urgent2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/urgent2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/urgent2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/urgent2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/urgent2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>