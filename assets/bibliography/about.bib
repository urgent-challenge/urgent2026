@inproceedings{zhangURGENTChallengeUniversality2024,
  title = {URGENT Challenge: Universality, Robustness, and Generalizability For Speech Enhancement},
  shorttitle = {URGENT Challenge},
  booktitle = {Interspeech 2024},
  author = {Zhang, Wangyou and Scheibler, Robin and Saijo, Kohei and Cornell, Samuele and Li, Chenda and Ni, Zhaoheng and Kumar, Anurag and Pirklbauer, Jan and Sach, Marvin and Watanabe, Shinji and Fingscheidt, Tim and Qian, Yanmin},
  year = {2024},
  eprint = {2406.04660},
  eprinttype = {arXiv},
  eprintclass = {cs, eess},
  pages = {4868--4872},
  doi = {10.21437/Interspeech.2024-1239},
  url = {http://arxiv.org/abs/2406.04660},
  urldate = {2024-10-08},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
}





@inproceedings{koheiurgent2025,
  title = {Interspeech 2025 URGENT Speech Enhancement Challenge},
  booktitle = {Interspeech 2026},
  author = {Saijo, Kohei and Zhang, Wangyou and  Cornell, Samuele and Scheibler, Robin and Li, Chenda and Ni, Zhaoheng and Kumar, Anurag and Sach, Marvin and Fu, Yihui and Wang, Wei and Fingscheidt, Tim  and Watanabe, Shinji},
  year=2026,
}

@inproceedings{zhang2025lessons,
  title={Lessons Learned From the URGENT 2024 Speech Enhancement Challenge},
  author={Zhang, Wangyou and Saijo, Kohei and Cornell, Samuele and Scheibler, Robin and Li, Chenda and Ni, Zhaoheng and Kumar, Anurag and Sach, Marvin and Wang, Wei and Fu, Yihui and others},
  booktitle={Interspeech},
  year={2025}
}

@book{loizouSpeechEnhancementTheory2007,
  title = {Speech Enhancement: Theory and Practice},
  shorttitle = {Speech Enhancement},
  author = {Loizou, Philipos C.},
  year = {2007},
  publisher = {CRC Press},
  location = {Boca Raton},
  doi = {10.1201/9781420015836},
  isbn = {978-0-429-13373-2},
  pagetotal = {632},
  file = {/Users/lichenda/Zotero/storage/NU83LA5N/Loizou - 2007 - Speech Enhancement Theory and Practice.pdf}
}


@article{wangSupervisedSpeechSeparation2018a,
  title = {Supervised Speech Separation Based on Deep Learning: An Overview},
  shorttitle = {Supervised Speech Separation Based on Deep Learning},
  author = {Wang, DeLiang and Chen, Jitong},
  year = {2018},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  shortjournal = {IEEE/ACM Trans. Audio Speech Lang. Process.},
  volume = {26},
  number = {10},
  pages = {1702--1726},
  issn = {2329-9290, 2329-9304},
  doi = {10.1109/TASLP.2018.2842159},
  url = {https://ieeexplore.ieee.org/document/8369155/},
  urldate = {2024-04-02},
  langid = {english},
  file = {/Users/lichenda/Zotero/storage/S9SWINML/Wang and Chen - 2018 - Supervised Speech Separation Based on Deep Learning An Overview.pdf}
}



@inproceedings{zhangPerformancePlateausComprehensive2024,
  title = {Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement},
  shorttitle = {Beyond Performance Plateaus},
  booktitle = {Proc. Interspeech 2024},
  author = {Zhang, Wangyou and Saijo, Kohei and Jung, Jee-weon and Li, Chenda and Watanabe, Shinji and Qian, Yanmin},
  year = {2024},
  pages = {1740--1744},
  doi = {10.21437/Interspeech.2024-1266},
  urldate = {2025-05-25},
}



@article{richterSpeechEnhancementDereverberation2023,
  title = {Speech Enhancement and Dereverberation With Diffusion-Based Generative Models},
  author = {Richter, Julius and Welker, Simon and Lemercier, Jean-Marie and Lay, Bunlong and Gerkmann, Timo},
  year = {2023},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  shortjournal = {IEEE/ACM Trans. Audio Speech Lang. Process.},
  volume = {31},
  pages = {2351--2364},
  issn = {2329-9290, 2329-9304},
  doi = {10.1109/TASLP.2023.3285241},
  url = {https://ieeexplore.ieee.org/document/10149431/},
  urldate = {2023-11-08},
  langid = {english},
  file = {/Users/lichenda/Zotero/storage/XEE4Z6KE/Richter et al. - 2023 - Speech Enhancement and Dereverberation With Diffusion-Based Generative Models.pdf;/Users/lichenda/Zotero/storage/STDA6NKS/10149431.html}
}

@inproceedings{rouxSDRHalfbakedWell2019,
  title = {SDR -- Half-baked or Well Done?},
  booktitle = {ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  author = {Roux, J. L. and Wisdom, S. and Erdogan, H. and Hershey, J. R.},
  year = {2019},
  month = may,
  pages = {626--630},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2019.8683855},
  keywords = {objective measure,signal-to-noise-ratio,source separation,speech enhancement},
  file = {/Users/lichenda/Zotero/storage/PUZKRACS/Roux 等。 - 2019 - SDR – Half-baked or Well Done.pdf;/Users/lichenda/Zotero/storage/2ZE4UWIY/8683855.html}
}

@article{luoConvTasNetSurpassingIdeal2019,
  title = {Conv-TasNet: Surpassing Ideal Time--Frequency Magnitude Masking for Speech Separation},
  shorttitle = {Conv-TasNet},
  author = {Luo, Y. and Mesgarani, N.},
  year = {2019},
  month = aug,
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {27},
  number = {8},
  pages = {1256--1266},
  issn = {2329-9304},
  doi = {10.1109/TASLP.2019.2915167},
  keywords = {Conv-TasNet system,Convolution,Decoding,deep learning,Deep learning,deep learning framework,end-to-end time-domain speech separation,fully convolutional time-domain audio separation network,ideal time-frequency magnitude masking,learning (artificial intelligence),linear codes,linear decoder,linear encoder,mixed signal,modified encoder representations,objective distortion measures,real-time,real-time speech separation applications,real-world speech processing technologies,signal representation,single-channel,single-channel speaker-independent speech separation methods,source separation,Source separation,speaker separation,Spectrogram,spectrograms,speech coding,Speech processing,speech separation systems,speech signal,speech waveform,speech waveform representation,stacked one-dimensional dilated convolutional blocks,subjective quality assessment,temporal convolutional network,three-speaker mixtures,time-domain,Time-domain analysis,time-frequency analysis,Time-frequency analysis,time-frequency representation,two-speaker speech separation,weighting functions},
  file = {/Users/lichenda/Zotero/storage/4BLQQK33/Luo 和 Mesgarani - 2019 - Conv-TasNet Surpassing Ideal Time–Frequency Magni.pdf;/Users/lichenda/Zotero/storage/M5SHJCMI/8707065.html}
}


@inproceedings{huDCCRNDeepComplex2020,
  title = {DCCRN: Deep Complex Convolution Recurrent Network for Phase-Aware Speech Enhancement},
  shorttitle = {DCCRN},
  booktitle = {Interspeech 2020},
  author = {Hu, Yanxin and Liu, Yun and Lv, Shubo and Xing, Mengtao and Zhang, Shimin and Fu, Yihui and Wu, Jian and Zhang, Bihong and Xie, Lei},
  year = {2020},
  pages = {2472--2476},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2020-2537},
  url = {https://www.isca-speech.org/archive/interspeech_2020/hu20g_interspeech.html},
  urldate = {2022-01-26},
  eventtitle = {Interspeech 2020},
  langid = {english},
  file = {/Users/lichenda/Zotero/storage/B5W2P5EF/Hu et al. - 2020 - DCCRN Deep Complex Convolution Recurrent Network .pdf}
}

@inproceedings{barkerThirdCHiMESpeech2015,
  title = {The Third ‘CHiME’ Speech Separation and Recognition Challenge: Dataset, Task and Baselines},
  shorttitle = {The Third ‘CHiME’ Speech Separation and Recognition Challenge},
  booktitle = {2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)},
  author = {Barker, Jon and Marxer, Ricard and Vincent, Emmanuel and Watanabe, Shinji},
  year = {2015},
  pages = {504--511},
  location = {Arizona, US},
  doi = {10.1109/ASRU.2015.7404837},
  url = {https://ieeexplore.ieee.org/abstract/document/7404837?casa_token=Gp-SnYl2Hj4AAAAA:twsjD3sjIblmI4dNjAQMNDJc2-TsnjLsVnvsH85yM00lQFrvJ9i-a7RGgQwDDrO-MAfimdU0},
  urldate = {2024-02-29},
  eventtitle = {2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)},
  keywords = {'CHiME' challenge,Arrays,microphone array,Microphones,Noise measurement,Noise-robust ASR,Speech,Speech recognition,Training,Training data},
  file = {/Users/lichenda/Zotero/storage/XNADBKMQ/Barker et al. - 2015 - The third ‘CHiME’ speech separation and recognition challenge Dataset, task and baselines.pdf;/Users/lichenda/Zotero/storage/S7W3NPLH/7404837.html}
}


@article{gonzalezEffectTrainingDataset2024a,
  title = {The Effect of Training Dataset Size on Discriminative and Diffusion-Based Speech Enhancement Systems},
  author = {Gonzalez, Philippe and Tan, Zheng-Hua and Østergaard, Jan and Jensen, Jesper and Alstrøm, Tommy Sonne and May, Tobias},
  year = {2024},
  journaltitle = {IEEE Signal Processing Letters},
  volume = {31},
  pages = {2225--2229},
  issn = {1558-2361},
  doi = {10.1109/LSP.2024.3449221},
  url = {https://ieeexplore.ieee.org/abstract/document/10645209},
  urldate = {2025-05-27},
  keywords = {Databases,diffusion models,discriminative models,Ear,Noise,Receivers,Reflection,Speech enhancement,Training,training data},
  file = {/Users/lichenda/Zotero/storage/Q2QWBWWV/Gonzalez et al. - 2024 - The Effect of Training Dataset Size on Discriminative and Diffusion-Based Speech Enhancement Systems.pdf}
}

@article{dubeyICASSP2023Deep2024,
  title = {ICASSP 2023 Deep Noise Suppression Challenge},
  author = {Dubey, Harishchandra and Aazami, Ashkan and Gopal, Vishak and Naderi, Babak and Braun, Sebastian and Cutler, Ross and Ju, Alex and Zohourian, Mehdi and Tang, Min and Golestaneh, Mehrsa and Aichner, Robert},
  year = {2024},
  journaltitle = {IEEE Open Journal of Signal Processing},
  volume = {5},
  pages = {725--737},
  issn = {2644-1322},
  doi = {10.1109/OJSP.2024.3378602},
  url = {https://ieeexplore.ieee.org/abstract/document/10474162},
  urldate = {2025-05-27},
  keywords = {Background noise,Computational modeling,Deep noise suppression,DNS challenge,Noise reduction,perceptual speech quality,personalized deep noise suppression,personalized P.835,Real-time systems,Speech enhancement,Speech recognition,target speech extraction,Training}
}


@article{xuRegressionApproachSpeech2015b,
  title = {A Regression Approach to Speech Enhancement Based on Deep Neural Networks},
  author = {Xu, Yong and Du, Jun and Dai, Li-Rong and Lee, Chin-Hui},
  year = {2015},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {23},
  number = {1},
  pages = {7--19},
  issn = {2329-9304},
  doi = {10.1109/TASLP.2014.2364452},
  url = {https://ieeexplore.ieee.org/abstract/document/6932438},
  urldate = {2025-05-27},
  keywords = {Deep neural networks (DNNs),dropout,global variance equalization,IEEE transactions,Noise,noise aware training,Noise measurement,noise reduction,nonstationary noise,Speech,speech enhancement,Speech enhancement,Training}
}

@inproceedings{luConditionalDiffusionProbabilistic2022,
  title = {Conditional Diffusion Probabilistic Model for Speech Enhancement},
  booktitle = {ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  author = {Lu, Yen-Ju and Wang, Zhong-Qiu and Watanabe, Shinji and Richard, Alexander and Yu, Cheng and Tsao, Yu},
  year = {2022},
  pages = {7402--7406},
  location = {Singapore},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9746901},
  url = {https://ieeexplore.ieee.org/abstract/document/9746901},
  urldate = {2023-12-17},
  eventtitle = {ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  keywords = {Adaptation models,Conferences,deep learning,diffusion probabilistic model,generative model,Signal processing,Signal processing algorithms,speech enhancement,Speech enhancement,Training,Training data},
  file = {/Users/lichenda/Zotero/storage/73LG7UGS/Lu et al. - 2022 - Conditional Diffusion Probabilistic Model for Speech Enhancement.pdf;/Users/lichenda/Zotero/storage/ZXPKTVIM/Lu et al. - 2022 - Conditional Diffusion Probabilistic Model for Speech Enhancement.pdf;/Users/lichenda/Zotero/storage/TNAHZLXZ/9746901.html}
}

@online{liDiffusionbasedGenerativeModeling2024,
  title = {Diffusion-Based Generative Modeling with Discriminative Guidance for Streamable Speech Enhancement},
  author = {Li, Chenda and Cornell, Samuele and Watanabe, Shinji and Qian, Yanmin},
  year = {2024},
  eprint = {2406.13471},
  eprinttype = {arXiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2406.13471},
  urldate = {2024-06-23},
  pubstate = {prepublished},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/lichenda/Zotero/storage/GH3GIWM6/Li et al. - 2024 - Diffusion-based Generative Modeling with Discriminative Guidance for Streamable Speech Enhancement.pdf;/Users/lichenda/Zotero/storage/BFZZEYRT/2406.html}
}

@inproceedings{valentini-botinhaoSpeechEnhancementNoiseRobust2016,
  title = {Speech Enhancement for a Noise-Robust Text-to-Speech Synthesis System Using Deep Recurrent Neural Networks},
  author = {Valentini-Botinhao, Cassia and Wang, Xin and Takaki, Shinji and Yamagishi, Junichi},
  year = {2016},
  pages = {352--356},
  doi = {10.21437/Interspeech.2016-159},
  url = {https://www.isca-archive.org/interspeech_2016/valentinibotinhao16_interspeech.html},
  urldate = {2025-05-27},
  eventtitle = {Proc. Interspeech 2016},
  file = {/Users/lichenda/Zotero/storage/G8ATDJVS/Valentini-Botinhao et al. - 2016 - Speech Enhancement for a Noise-Robust Text-to-Speech Synthesis System Using Deep Recurrent Neural Ne.pdf}
}


@inproceedings{yuEfficientMonauralSpeech2023,
  title = {Efficient Monaural Speech Enhancement with Universal Sample Rate Band-Split RNN},
  booktitle = {ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  author = {Yu, Jianwei and Luo, Yi},
  year = {2023},
  pages = {1--5},
  doi = {10.1109/ICASSP49357.2023.10096020},
  eventtitle = {ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  keywords = {Bandwidth,Benchmark testing,Computational efficiency,Computational modeling,Dynamic complexity,Speech enhancement,Task analysis,Training,Universal sample rate},
  file = {/Users/lichenda/Zotero/storage/8992XF6P/Yu and Luo - 2023 - Efficient Monaural Speech Enhancement with Univers.pdf;/Users/lichenda/Zotero/storage/A3MGMWBX/Yu and Luo - 2023 - Efficient Monaural Speech Enhancement with Univers.pdf;/Users/lichenda/Zotero/storage/V396NZD6/stamp.html}
}


@inproceedings{yangTFPSNetTimeFrequencyDomain2022,
  title = {TFPSNet: Time-Frequency Domain Path Scanning Network for Speech Separation},
  shorttitle = {TFPSNet},
  booktitle = {ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  author = {Yang, Lei and Liu, Wei and Wang, Weiqin},
  year = {2022},
  pages = {6842--6846},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9747554},
  eventtitle = {ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  keywords = {Acoustics,Conferences,deep learning,Deep learning,Noise measurement,Signal processing,source separation,speech separation,T-F domain,Time-frequency analysis,transformer,Transformers},
  file = {/Users/lichenda/Zotero/storage/BQ9KAEUD/Yang et al. - 2022 - TFPSNet Time-Frequency Domain Path Scanning Netwo.pdf;/Users/lichenda/Zotero/storage/WMCL8E9M/9747554.html}
}


@inproceedings{wangTFGridNetMakingTimeFrequency2023,
  title={{TF-GridNet}: Making Time-Frequency Domain Models Great Again for Monaural Speaker Separation},
  author={Wang, Zhong-Qiu and Cornell, Samuele and Choi, Shukjae and Lee, Younglo and Kim, Byeong-Yeol and Watanabe, Shinji},
  booktitle = {ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year={2023},
}

@inproceedings{chenComplexityScalingSpeech2024,
  title = {Complexity Scaling for Speech Denoising},
  booktitle = {ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  author = {Chen, Hangting and Yu, Jianwei and Weng, Chao},
  year = {2024},
  pages = {12276--12280},
  issn = {2379-190X},
  doi = {10.1109/ICASSP48485.2024.10448061},
  url = {https://ieeexplore.ieee.org/abstract/document/10448061},
  urldate = {2025-05-31},
  eventtitle = {ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  keywords = {complexity scaling,Computational efficiency,Computational modeling,Computer architecture,multi-path transformer,neural network,Neural networks,Noise reduction,Signal processing,Speech denoising,Transformers},
  file = {/Users/lichenda/Zotero/storage/9UC949AM/Chen et al. - 2024 - Complexity Scaling for Speech Denoising.pdf}
}


@article{luoMusicSourceSeparation2023,
  title = {Music Source Separation With Band-Split RNN},
  author = {Luo, Yi and Yu, Jianwei},
  year = {2023},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {31},
  pages = {1893--1901},
  issn = {2329-9304},
  doi = {10.1109/TASLP.2023.3271145},
  url = {https://ieeexplore.ieee.org/abstract/document/10121418},
  urldate = {2025-02-23},
  eventtitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  keywords = {band-split RNN,Data models,deep learning,Estimation,Instruments,Multiple signal classification,Music separation,Pipelines,Spectrogram,Speech enhancement},
  file = {/Users/lichenda/Zotero/storage/2R439VTG/Luo和Yu - 2023 - Music Source Separation With Band-Split RNN.pdf}
}


@article{Lessons-Zhang2025,
  title={Lessons Learned from the URGENT 2024 Speech Enhancement Challenge},
  author={Zhang, Wangyou and Saijo, Kohei and Cornell, Samuele and Scheibler, Robin and Li, Chenda and Ni, Zhaoheng and Kumar, Anurag and Sach, Marvin and Wang, Wei and Fu, Yihui and Watanabe, Shinji and Fingscheidt, Tim and Qian, Yanmin},
  journal={Accepted by Interspeech},
  year={2025},
}

@inproceedings{ardilaCommonVoiceMassivelyMultilingual2020,
  title = {Common Voice: A Massively-Multilingual Speech Corpus},
  shorttitle = {Common Voice},
  booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},
  author = {Ardila, Rosana and Branson, Megan and Davis, Kelly and Kohler, Michael and Meyer, Josh and Henretty, Michael and Morais, Reuben and Saunders, Lindsay and Tyers, Francis and Weber, Gregor},
  editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
  year = {2020},
  pages = {4218--4222},
  publisher = {European Language Resources Association},
  location = {Marseille, France},
  url = {https://aclanthology.org/2020.lrec-1.520/},
  urldate = {2025-05-31},
  eventtitle = {LREC 2020},
  isbn = {979-10-95546-34-4},
  langid = {english},
  file = {/Users/lichenda/Zotero/storage/7VHA9IMD/Ardila et al. - 2020 - Common Voice A Massively-Multilingual Speech Corpus.pdf}
}

@article{kearns2014librivox,
  title={Librivox: Free public domain audiobooks},
  author={Kearns, Jodi},
  journal={Reference Reviews},
  volume={28},
  number={1},
  pages={7--8},
  year={2014},
  publisher={Emerald group publishing limited}
}


@inproceedings{veauxVoiceBankCorpus2013,
  title = {The Voice Bank Corpus: Design, Collection and Data Analysis of a Large Regional Accent Speech Database},
  shorttitle = {The Voice Bank Corpus},
  booktitle = {2013 International Conference Oriental COCOSDA Held Jointly with 2013 Conference on Asian Spoken Language Research and Evaluation (O-COCOSDA/CASLRE)},
  author = {Veaux, Christophe and Yamagishi, Junichi and King, Simon},
  year = {2013},
  pages = {1--4},
  doi = {10.1109/ICSDA.2013.6709856},
  url = {https://ieeexplore.ieee.org/abstract/document/6709856},
  urldate = {2025-05-31},
  eventtitle = {2013 International Conference Oriental COCOSDA Held Jointly with 2013 Conference on Asian Spoken Language Research and Evaluation (O-COCOSDA/CASLRE)},
  keywords = {Corpus Design,Databases,Educational institutions,Hidden Markov models,Optimization,Recruitment,Speech,Speech synthesis,Speech Synthesis,Text Selection,Voice Banking}
}



@inproceedings{zenLibriTTSCorpusDerived2019,
  title = {LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech},
  shorttitle = {LibriTTS},
  booktitle = {Proc. Interspeech 2019},
  author = {Zen, Heiga and Dang, Viet and Clark, Rob and Zhang, Yu and Weiss, Ron J. and Jia, Ye and Chen, Zhifeng and Wu, Yonghui},
  year = {2019},
  pages = {1526--1530},
  doi = {10.21437/Interspeech.2019-2441},
  urldate = {2024-09-30},
  file = {/Users/lichenda/Zotero/storage/7KCPV44L/Zen et al. - 2019 - LibriTTS A Corpus Derived from LibriSpeech for Text-to-Speech.pdf}
}


@inproceedings{richterEARSAnechoicFullband2024,
  title = {EARS: An Anechoic Fullband Speech Dataset Benchmarked for Speech Enhancement and Dereverberation},
  shorttitle = {EARS},
  author = {Richter, Julius and Wu, Yi-Chiao and Krenn, Steven and Welker, Simon and Lay, Bunlong and Watanabe, Shinji and Richard, Alexander and Gerkmann, Timo},
  year = {2024},
  pages = {4873--4877},
  doi = {10.21437/Interspeech.2024-153},
  url = {https://www.isca-archive.org/interspeech_2024/richter24_interspeech.html#},
  urldate = {2025-05-31},
  eventtitle = {Proc. Interspeech 2024}
}



@inproceedings{pratapMLSLargeScaleMultilingual2020,
  title = {MLS: A Large-Scale Multilingual Dataset for Speech Research},
  shorttitle = {MLS},
  booktitle = {Proc. Interspeech 2020},
  author = {Pratap, Vineel and Xu, Qiantong and Sriram, Anuroop and Synnaeve, Gabriel and Collobert, Ronan},
  year = {2020},
  pages = {2757--2761},
  doi = {10.21437/Interspeech.2020-2826},
  urldate = {2025-05-31},
  file = {/Users/lichenda/Zotero/storage/HR3G4IRX/Pratap et al. - 2020 - MLS A Large-Scale Multilingual Dataset for Speech Research.pdf}
}


@dataset{garofolojohns.CSRIWSJ0Complete2007,
  title = {CSR-I (WSJ0) Complete},
  author = {{Garofolo, John S.} and {Graff, David} and {Paul, Doug} and {Pallett, David}},
  year = {2007},
  pages = {9542041 KB},
  publisher = {Linguistic Data Consortium},
  doi = {10.35111/EWKM-CG47},
  url = {https://catalog.ldc.upenn.edu/LDC93S6A},
  urldate = {2024-02-29},
}


@techreport{fevotteBSS_EVALToolboxUser2005,
  type = {Report},
  title = {BSS\_EVAL Toolbox User Guide -- Revision 2.0},
  author = {F{\'e}votte, C{\'e}dric and Gribonval, R{\'e}mi and Vincent, Emmanuel},
  year = {2005},
  pages = {19},
  urldate = {2025-06-01},
  abstract = {This document is meant to help you use the BSS EVAL toolbox, which implements some criteria for performance measurement in (blind) source separation. The toolbox -- which is distributed under the terms of the GNU GENERAL PUBLIC LICENSE as a set of Matlab routines --can be downloaded at the address http://www.irisa.fr/metiss/bss\_eval/. The purpose of this toolbox is to measure the performance of various source separation algorithms in an evaluation framework where the original sources, and perhaps even the noise that perturbed the mixture, are available for comparison.},
  langid = {english},
  file = {/Users/lichenda/Zotero/storage/IK2VW6PR/Févotte et al. - 2005 - BSS_EVAL Toolbox User Guide -- Revision 2.0.pdf}
}

@inproceedings{kubichekMelcepstralDistanceMeasure1993,
  title = {Mel-Cepstral Distance Measure for Objective Speech Quality Assessment},
  booktitle = {Proceedings of IEEE Pacific Rim Conference on Communications Computers and Signal Processing},
  author = {Kubichek, R.},
  year = {1993},
  month = may,
  volume = {1},
  pages = {125-128 vol.1},
  doi = {10.1109/PACRIM.1993.407206},
  urldate = {2024-02-29},
  abstract = {The author proposes a perceptually motivated modification to the cepstral distance measure (CD) based on the mel frequency scale and critical-band filtering. The new objective parameter is referred to as the mel cepstral distance (MCD). The author measures and compares the performance of the CD and MCD algorithms by applying them to a dataset representing low-bit-rate code-excited linear prediction (CELP)-coded speech with simulated channel conditions. The improvement in correlation with subjective DAM scores indicates that critical band filtering (and frequency warping) allows better modeling of perceived quality.{$<>$}},
  keywords = {Cepstral analysis,Distortion measurement,Ear,Filters,Frequency measurement,Humans,Nonlinear distortion,Psychoacoustic models,Quality assessment,Speech},
  file = {/Users/lichenda/Zotero/storage/4XQC9QYF/Kubichek - 1993 - Mel-cepstral distance measure for objective speech quality assessment.pdf}
}

@inproceedings{rixPerceptualEvaluationSpeech2001,
  title = {Perceptual Evaluation of Speech Quality (PESQ)-a New Method for Speech Quality Assessment of Telephone Networks and Codecs},
  booktitle = {2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.01CH37221)},
  author = {Rix, A. W. and Beerends, J. G. and Hollier, M. P. and Hekstra, A. P.},
  year = {2001},
  month = may,
  volume = {2},
  pages = {749-752 vol.2},
  address = {Utah, USA},
  issn = {1520-6149},
  doi = {10.1109/ICASSP.2001.941023},
  abstract = {Previous objective speech quality assessment models, such as bark spectral distortion (BSD), the perceptual speech quality measure (PSQM), and measuring normalizing blocks (MNB), have been found to be suitable for assessing only a limited range of distortions. A new model has therefore been developed for use across a wider range of network conditions, including analogue connections, codecs, packet loss and variable delay. Known as perceptual evaluation of speech quality (PESQ), it is the result of integration of the perceptual analysis measurement system (PAMS) and PSQM99, an enhanced version of PSQM. PESQ is expected to become a new ITU-T recommendation P.862, replacing P.861 which specified PSQM and MNB.},
  keywords = {Degradation,Delay effects,Distortion measurement,Nonlinear distortion,Nonlinear filters,Quality assessment,Signal processing,Speech analysis,Speech codecs,Telephony},
  file = {/Users/lichenda/Zotero/storage/E5X2ML5T/941023.html}
}


@inproceedings{liuSeparateWhatYou2022,
  title = {Separate What You Describe: Language-Queried Audio Source Separation},
  shorttitle = {Separate What You Describe},
  booktitle = {Proc. Interspeech},
  author = {Liu, Xubo and Liu, Haohe and Kong, Qiuqiang and Mei, Xinhao and Zhao, Jinzheng and Huang, Qiushi and Plumbley, Mark D. and Wang, Wenwu},
  year = {2022},
  pages = {1801--1805},
  doi = {10.21437/Interspeech.2022-10894},
  urldate = {2025-02-23},
  file = {/Users/lichenda/Zotero/storage/ZIUT4CYA/Liu 等 - 2022 - Separate What You Describe Language-Queried Audio Source Separation.pdf}
}


@inproceedings{mittagNISQADeepCNNSelfAttention2021,
  title = {NISQA: A Deep CNN-Self-Attention Model for Multidimensional Speech Quality Prediction with Crowdsourced Datasets},
  shorttitle = {NISQA},
  booktitle = {Proc. Interspeech 2021},
  author = {Mittag, Gabriel and Naderi, Babak and Chehadi, Assmaa and M{\"o}ller, Sebastian},
  year = {2021},
  pages = {2127--2131},
  doi = {10.21437/Interspeech.2021-299},
  urldate = {2024-02-29},
  file = {/Users/lichenda/Zotero/storage/E7IHKTLA/Mittag et al. - 2021 - NISQA A Deep CNN-Self-Attention Model for Multidimensional Speech Quality Prediction with Crowdsour.pdf}
}



@inproceedings{reddyDnsmosNonIntrusivePerceptual2021,
  title = {Dnsmos: A Non-Intrusive Perceptual Objective Speech Quality Metric to Evaluate Noise Suppressors},
  shorttitle = {Dnsmos},
  booktitle = {ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  author = {Reddy, Chandan K A and Gopal, Vishak and Cutler, Ross},
  year = {2021},
  month = jun,
  pages = {6493--6497},
  issn = {2379-190X},
  doi = {10.1109/ICASSP39728.2021.9414878},
  keywords = {Acoustics,Conferences,Correlation,Deep Noise Suppressor,Measurement,Metric,Noise reduction,Objective Metric,Perceptual Speech Quality,Signal processing,Signal processing algorithms,Speech},
  file = {/Users/lichenda/Zotero/storage/S5NF78L8/Reddy et al. - 2021 - Dnsmos A Non-Intrusive Perceptual Objective Speec.pdf;/Users/lichenda/Zotero/storage/Z63T58E2/9414878.html}
}

@article{risteaICASSP2024Speech2025,
  title = {ICASSP 2024 Speech Signal Improvement Challenge},
  author = {Ristea, Nicolae-C{\u a}t{\u a}lin and Naderi, Babak and Saabas, Ando and Cutler, Ross and Braun, Sebastian and Branets, Solomiya},
  year = {2025},
  journal = {IEEE Open Journal of Signal Processing},
  volume = {6},
  pages = {238--246},
  issn = {2644-1322},
  doi = {10.1109/OJSP.2025.3526550},
  urldate = {2025-06-01},
  keywords = {Background noise,Correlation,deep learning,Distortion,Measurement,Noise,Noise reduction,Quality assessment,Reverberation,Signal processing algorithms,Speech enhancement,speech quality assessment,subjective testing}
}

@inproceedings{kumarTorchaudioSquimReferenceLessSpeech2023,
  title = {Torchaudio-Squim: Reference-Less Speech Quality and Intelligibility Measures in Torchaudio},
  shorttitle = {Torchaudio-Squim},
  booktitle = {ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  author = {Kumar, Anurag and Tan, Ke and Ni, Zhaoheng and Manocha, Pranay and Zhang, Xiaohui and Henderson, Ethan and Xu, Buye},
  year = {2023},
  month = jun,
  pages = {1--5},
  issn = {2379-190X},
  doi = {10.1109/ICASSP49357.2023.10096680},
  urldate = {2025-06-01},
  keywords = {Computational modeling,Current measurement,Deep learning,Estimation,mean opinion score,Measurement,Neural networks,PESQ,SI-SDR,Signal processing,speech intelligibility,Speech quality,STOI}
}


@inproceedings{saekiUTMOSUTokyoSaruLabSystem2022,
  title = {UTMOS: UTokyo-SaruLab System for VoiceMOS Challenge 2022},
  shorttitle = {UTMOS},
  booktitle = {Proc. Interspeech 2022},
  author = {Saeki, Takaaki and Xin, Detai and Nakata, Wataru and Koriyama, Tomoki and Takamichi, Shinnosuke and Saruwatari, Hiroshi},
  year = {2022},
  pages = {4521--4525},
  doi = {10.21437/Interspeech.2022-439},
  urldate = {2024-10-01},
  file = {/Users/lichenda/Zotero/storage/M9QQEITY/Saeki et al. - 2022 - UTMOS UTokyo-SaruLab System for VoiceMOS Challenge 2022.pdf}
}




@inproceedings{leeFlowSEFlowMatchingbased2025,
  title = {FlowSE: Flow Matching-based Speech Enhancement},
  shorttitle = {FlowSE},
  booktitle = {ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  author = {Lee, Seonggyu and Cheong, Sein and Han, Sangwook and Shin, Jong Won},
  year = {2025},
  month = apr,
  pages = {1--5},
  issn = {2379-190X},
  doi = {10.1109/ICASSP49660.2025.10888274},
  urldate = {2025-06-01},
  keywords = {Computational modeling,diffusion model,Diffusion models,Diffusion processes,flow matching,generative model,Kernel,Noise measurement,Perturbation methods,Signal processing,speech enhancement,Speech enhancement,Tuning,Vectors}
}

@inproceedings{cumlinDNSMOSProReducedSize2024,
  title = {DNSMOS Pro: A Reduced-Size DNN for Probabilistic MOS of Speech},
  shorttitle = {DNSMOS Pro},
  booktitle = {Proc. Interspeech 2024},
  author = {Cumlin, Fredrik and Liang, Xinyu and Ungureanu, Victor and K. A. Reddy, Chandan and Sch{\"u}ldt, Christian and Chatterjee, Saikat},
  year = {2024},
  pages = {4818--4822},
  doi = {10.21437/Interspeech.2024-478},
  urldate = {2025-06-03}
}

@inproceedings{fuSelfSupervisedSpeechQuality2023,
  title = {Self-Supervised Speech Quality Estimation and Enhancement Using Only Clean Speech},
  booktitle = {The Twelfth International Conference on Learning Representations},
  author = {Fu, Szu-Wei and Hung, Kuo-Hsuan and Tsao, Yu and Wang, Yu-Chiang Frank},
  year = {2023},
  month = oct,
  urldate = {2025-06-03},
  langid = {english},
  file = {/Users/lichenda/Zotero/storage/SKVQUUJW/Fu et al. - 2023 - Self-Supervised Speech Quality Estimation and Enhancement Using Only Clean Speech.pdf}
}


@inproceedings{stahlDistillationPruningScalable2025,
  title = {Distillation and Pruning for Scalable Self-Supervised Representation-Based Speech Quality Assessment},
  booktitle = {ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  author = {Stahl, Benjamin and Gamper, Hannes},
  year = {2025},
  month = apr,
  pages = {1--5},
  issn = {2379-190X},
  doi = {10.1109/ICASSP49660.2025.10888007},
  urldate = {2025-06-03},
  keywords = {Acoustics,Correlation,Data models,Quality assessment,quality of experience,Signal processing,speech processing,Speech processing},
  file = {/Users/lichenda/Zotero/storage/VSH46QK5/Stahl and Gamper - 2025 - Distillation and Pruning for Scalable Self-Supervised Representation-Based Speech Quality Assessment.pdf}
}


@article{vincentPerformanceMeasurementBlind2006,
  title = {Performance Measurement in Blind Audio Source Separation},
  author = {Vincent, E. and Gribonval, R. and Fevotte, C.},
  year = {2006},
  month = jul,
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {14},
  number = {4},
  pages = {1462--1469},
  issn = {1558-7924},
  doi = {10.1109/TSA.2005.858005},
  keywords = {Additive noise,Audio source separation,Data mining,Distortion measurement,Energy measurement,evaluation,Filters,Image analysis,Independent component analysis,Interference,measure,Microphones,performance,quality,Source separation},
  file = {/Users/lichenda/Zotero/storage/6YHL7IF3/Vincent 等。 - 2006 - Performance measurement in blind audio source sepa.pdf;/Users/lichenda/Zotero/storage/B5Q3CKED/1643671.html}
}

@article{jensenAlgorithmPredictingIntelligibility2016,
  title = {An Algorithm for Predicting the Intelligibility of Speech Masked by Modulated Noise Maskers},
  author = {Jensen, Jesper and Taal, Cees H.},
  year = {2016},
  month = nov,
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {24},
  number = {11},
  pages = {2009--2022},
  issn = {2329-9304},
  doi = {10.1109/TASLP.2016.2585878},
  urldate = {2024-02-29},
  keywords = {Correlation,Indexes,Modulated noise sources,Modulation,Noise measurement,noise reduction,objective distortion measures,Prediction algorithms,Speech,speech enhancement,speech intelligibility prediction,Speech processing},
  file = {/Users/lichenda/Zotero/storage/GH872Q4S/Jensen and Taal - 2016 - An Algorithm for Predicting the Intelligibility of Speech Masked by Modulated Noise Maskers.pdf}
}

@article{grayDistanceMeasuresSpeech1976,
  title = {Distance Measures for Speech Processing},
  author = {Gray, A. and Markel, J.},
  year = {1976},
  month = oct,
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {24},
  number = {5},
  pages = {380--391},
  issn = {0096-3518},
  doi = {10.1109/TASSP.1976.1162849},
  urldate = {2024-03-02},
  keywords = {Autocorrelation,Cepstral analysis,Euclidean distance,Nonlinear filters,Oral communication,Root mean square,Speech analysis,Speech processing,Speech recognition,Testing},
  file = {/Users/lichenda/Zotero/storage/ABJY2QIV/Gray and Markel - 1976 - Distance measures for speech processing.pdf;/Users/lichenda/Zotero/storage/EGAK2V7Z/1162849.html}
}


@inproceedings{lipmanFlowMatchingGenerative2023,
  title = {Flow Matching for Generative Modeling},
  booktitle = {The Eleventh International Conference on Learning Representations},
  author = {Lipman, Yaron and Chen, Ricky T. Q. and {Ben-Hamu}, Heli and Nickel, Maximilian and Le, Matthew},
  year = {2022},
  month = sep,
  urldate = {2025-06-05},
  langid = {english},
  file = {/Users/lichenda/Zotero/storage/H89SHP3T/Lipman et al. - 2022 - Flow Matching for Generative Modeling.pdf}
}


@inproceedings{songScoreBasedGenerativeModeling2021,
  title = {Score-Based Generative Modeling through Stochastic Differential Equations},
  booktitle = {9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021},
  author = {Song, Yang and {Sohl-Dickstein}, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  year = {2021},
  publisher = {OpenReview.net},
  urldate = {2024-02-28}
}


@inproceedings{liESPnetSEEndEndSpeech2021,
  title = {ESPnet-SE: End-To-End Speech Enhancement and Separation Toolkit Designed for ASR Integration},
  shorttitle = {ESPnet-SE},
  booktitle = {2021 IEEE Spoken Language Technology Workshop (SLT)},
  author = {Li, Chenda and Shi, Jing and Zhang, Wangyou and Subramanian, Aswin Shanmugam and Chang, Xuankai and Kamo, Naoyuki and Hira, Moto and Hayashi, Tomoki and Boeddeker, Christoph and Chen, Zhuo and Watanabe, Shinji},
  year = {2021},
  month = jan,
  pages = {785--792},
  doi = {10.1109/SLT48900.2021.9383615},
  keywords = {Benchmark testing,end-to-end,Feature extraction,Open-source,Optimization,Pipelines,source separation,Source separation,speech enhancement,Speech enhancement,speech recognition,Training},
  file = {/Users/lichenda/Zotero/storage/BS3T7P3C/Li et al. - 2021 - ESPnet-SE End-To-End Speech Enhancement and Separation Toolkit Designed for ASR Integration.pdf}
}


@article{taalAlgorithmIntelligibilityPrediction2011,
  title = {An Algorithm for Intelligibility Prediction of Time--Frequency Weighted Noisy Speech},
  author = {Taal, Cees H. and Hendriks, Richard C. and Heusdens, Richard and Jensen, Jesper},
  year = {2011},
  month = sep,
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {19},
  number = {7},
  pages = {2125--2136},
  issn = {1558-7924},
  doi = {10.1109/TASL.2011.2114881},
  urldate = {2025-06-05},
  keywords = {Correlation,Noise measurement,Noise reduction,objective measure,Signal to noise ratio,Speech,speech enhancement,speech intelligibility prediction,Speech processing,Time frequency analysis},
  file = {/Users/lichenda/Zotero/storage/4NG2XDLN/Taal et al. - 2011 - An Algorithm for Intelligibility Prediction of Time–Frequency Weighted Noisy Speech.pdf}
}


@inproceedings{luLowDistortionMultiChannelSpeech2022,
  title = {Towards Low-Distortion Multi-Channel Speech Enhancement: The ESPNET-Se Submission to the L3DAS22 Challenge},
  shorttitle = {Towards Low-Distortion Multi-Channel Speech Enhancement},
  booktitle = {ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  author = {Lu, Yen-Ju and Cornell, Samuele and Chang, Xuankai and Zhang, Wangyou and Li, Chenda and Ni, Zhaoheng and Wang, Zhong-Qiu and Watanabe, Shinji},
  year = {2022},
  month = may,
  pages = {9201--9205},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9747146},
  keywords = {Array signal processing,beamforming,deep learning,Measurement,multi-channel speech enhancement,multi-microphone complex spectral mapping,Neural networks,Pipelines,Speech enhancement,Three-dimensional displays,Wiener filters},
  file = {/Users/lichenda/Zotero/storage/8IMN7LU6/Lu et al. - 2022 - Towards Low-Distortion Multi-Channel Speech Enhanc.pdf;/Users/lichenda/Zotero/storage/JN475VBY/9747146.html}
}


@article{zhang2024scale,
  title={Scale This, Not That: Investigating Key Dataset Attributes for Efficient Speech Enhancement Scaling},
  author={Zhang, Leying and Zhang, Wangyou and Li, Chenda and Qian, Yanmin},
  journal={arXiv preprint arXiv:2412.14890},
  year={2024}
}

@inproceedings{luESPnetSESpeechEnhancement2022,
  title = {ESPnet-SE++: Speech Enhancement for Robust Speech Recognition, Translation, and Understanding},
  shorttitle = {ESPnet-SE++},
  booktitle = {Interspeech 2022},
  author = {Lu, Yen-Ju and Chang, Xuankai and Li, Chenda and Zhang, Wangyou and Cornell, Samuele and Ni, Zhaoheng and Masuyama, Yoshiki and Yan, Brian and Scheibler, Robin and Wang, Zhong-Qiu and Tsao, Yu and Qian, Yanmin and Watanabe, Shinji},
  year = {2022},
  month = sep,
  pages = {5458--5462},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2022-10727},
  urldate = {2023-06-14},
  langid = {english},
  file = {/Users/lichenda/Zotero/storage/B84G2S2L/Lu et al. - 2022 - ESPnet-SE++ Speech Enhancement for Robust Speech .pdf}
}

@inproceedings{yangTorchaudioBuildingBlocks2022,
  title = {Torchaudio: Building Blocks for Audio and Speech Processing},
  shorttitle = {Torchaudio},
  booktitle = {ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  author = {Yang, Yao-Yuan and Hira, Moto and Ni, Zhaoheng and Astafurov, Artyom and Chen, Caroline and Puhrsch, Christian and Pollack, David and Genzel, Dmitriy and Greenberg, Donny and Yang, Edward Z. and Lian, Jason and Hwang, Jeff and Chen, Ji and Goldsborough, Peter and Narenthiran, Sean and Watanabe, Shinji and Chintala, Soumith and {Quenneville-B{\'e}lair}, Vincent},
  year = {2022},
  month = may,
  pages = {6982--6986},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9747236},
  urldate = {2025-07-05},
  abstract = {This document describes version 0.10 of TorchAudio: building blocks for machine learning applications in the audio and speech processing domain. The objective of TorchAudio is to accelerate the development and deployment of machine learning applications for researchers and engineers by providing off-the-shelf building blocks. The building blocks are designed to be GPU-compatible, automatically differentiable, and production-ready. TorchAudio can be easily installed from Python Package Index repository and the source code is publicly available under a BSD-2-Clause License (as of September 2021) at https://github.com/pytorch/audio. In this document, we provide an overview of the design principles, functionalities, and benchmarks of TorchAudio. We also benchmark our implementation of several audio and speech operations and models. We verify through the benchmarks that our implementations of various operations and models are valid and perform similarly to other publicly available implementations.},
  keywords = {Audio Processing,Benchmark testing,Indexes,Machine learning,Open source software,Open-Source Toolkit,Signal processing,Speech recognition,Speech Recognition,Text-to-Speech},
  file = {/Users/lichenda/Zotero/storage/AKYREV5I/Yang et al. - 2022 - Torchaudio Building Blocks for Audio and Speech Processing.pdf;/Users/lichenda/Zotero/storage/QC3FDUAW/9747236.html}
}


@inproceedings{watanabeCHiME6ChallengeTackling2020a,
  title = {CHiME-6 Challenge: Tackling Multispeaker Speech Recognition for Unsegmented Recordings},
  shorttitle = {CHiME-6 Challenge},
  booktitle = {6th International Workshop on Speech Processing in Everyday Environments (CHiME 2020)},
  author = {Watanabe, Shinji and Mandel, Michael and Barker, Jon and Vincent, Emmanuel and Arora, Ashish and Chang, Xuankai and Khudanpur, Sanjeev and Manohar, Vimal and Povey, Daniel and Raj, Desh and Snyder, David and Subramanian, Aswin Shanmugam and Trmal, Jan and Yair, Bar Ben and Boeddeker, Christoph and Ni, Zhaoheng and Fujita, Yusuke and Horiguchi, Shota and Kanda, Naoyuki and Yoshioka, Takuya and Ryant, Neville},
  year = {2020},
  month = may,
  pages = {1--7},
  publisher = {ISCA},
  doi = {10.21437/CHiME.2020-1},
  urldate = {2023-03-05},
  langid = {english},
  file = {/Users/lichenda/Zotero/storage/5XDINEAZ/Watanabe et al. - 2020 - CHiME-6 Challenge Tackling Multispeaker Speech Re.pdf}
}


@misc{cornellDCASE2024Task2024,
  title = {DCASE 2024 Task 4: Sound Event Detection with Heterogeneous Data and Missing Labels},
  shorttitle = {DCASE 2024 Task 4},
  author = {Cornell, Samuele and Ebbers, Janek and Douwes, Constance and {Mart{\'i}n-Morat{\'o}}, Irene and Harju, Manu and Mesaros, Annamaria and Serizel, Romain},
  year = {2024},
  month = jun,
  number = {arXiv:2406.08056},
  eprint = {2406.08056},
  primaryclass = {eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.08056},
  urldate = {2025-07-05},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/lichenda/Zotero/storage/AR8TM4HL/Cornell et al. - 2024 - DCASE 2024 Task 4 Sound Event Detection with Heterogeneous Data and Missing Labels.pdf;/Users/lichenda/Zotero/storage/ASWCRB5P/2406.html}
}

@misc{liLessMoreData2025,
  title = {Less Is More: Data Curation Matters in Scaling Speech Enhancement},
  shorttitle = {Less Is More},
  author = {Li, Chenda and Zhang, Wangyou and Wang, Wei and Scheibler, Robin and Saijo, Kohei and Cornell, Samuele and Fu, Yihui and Sach, Marvin and Ni, Zhaoheng and Kumar, Anurag and Fingscheidt, Tim and Watanabe, Shinji and Qian, Yanmin},
  year = {2025},
  month = jun,
  number = {arXiv:2506.23859},
  eprint = {2506.23859},
  primaryclass = {eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.23859},
  urldate = {2025-07-01},
  abstract = {The vast majority of modern speech enhancement systems rely on data-driven neural network models. Conventionally, larger datasets are presumed to yield superior model performance, an observation empirically validated across numerous tasks in other domains. However, recent studies reveal diminishing returns when scaling speech enhancement data. We focus on a critical factor: prevalent quality issues in ``clean'' training labels within large-scale datasets. This work re-examines this phenomenon and demonstrates that, within large-scale training sets, prioritizing high-quality training data is more important than merely expanding the data volume. Experimental findings suggest that models trained on a carefully curated subset of 700 hours can outperform models trained on the 2,500-hour full dataset. This outcome highlights the crucial role of data curation in scaling speech enhancement systems effectively.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/lichenda/Zotero/storage/CSYCUI3Q/Li et al. - 2025 - Less is More Data Curation Matters in Scaling Speech Enhancement.pdf;/Users/lichenda/Zotero/storage/8H2YZQ7S/2506.html}
}

@misc{wangURGENTPKPerceptuallyAlignedRanking2025,
  title = {URGENT-PK: Perceptually-Aligned Ranking Model Designed for Speech Enhancement Competition},
  shorttitle = {URGENT-PK},
  author = {Wang, Jiahe and Li, Chenda and Wang, Wei and Zhang, Wangyou and Cornell, Samuele and Sach, Marvin and Scheibler, Robin and Saijo, Kohei and Fu, Yihui and Ni, Zhaoheng and Kumar, Anurag and Fingscheidt, Tim and Watanabe, Shinji and Qian, Yanmin},
  year = {2025},
  month = jun,
  number = {arXiv:2506.23874},
  eprint = {2506.23874},
  primaryclass = {eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.23874},
  urldate = {2025-07-01},
  abstract = {The Mean Opinion Score (MOS) is fundamental to speech quality assessment. However, its acquisition requires significant human annotation. Although deep neural network approaches, such as DNSMOS and UTMOS, have been developed to predict MOS to avoid this issue, they often suffer from insufficient training data. Recognizing that the comparison of speech enhancement (SE) systems prioritizes a reliable system comparison over absolute scores, we propose URGENT-PK, a novel ranking approach leveraging pairwise comparisons. URGENT-PK takes homologous enhanced speech pairs as input to predict relative quality rankings. This pairwise paradigm efficiently utilizes limited training data, as all pairwise permutations of multiple systems constitute a training instance. Experiments across multiple open test sets demonstrate URGENT-PK's superior system-level ranking performance over state-of-the-art baselines, despite its simple network architecture and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/lichenda/Zotero/storage/HP2FVHLQ/Wang et al. - 2025 - URGENT-PK Perceptually-Aligned Ranking Model Designed for Speech Enhancement Competition.pdf;/Users/lichenda/Zotero/storage/N4KNQXGS/2506.html}
}

@misc{kodali_radha_mohan_bansal_2022,
	title={Non-Native Children English Speech (NNCES) Corpus},
	url={https://www.kaggle.com/dsv/4416485},
	DOI={10.34740/KAGGLE/DSV/4416485},
	publisher={Kaggle},
	author={Kodali, Radha and Mohan, Bansal},
	year={2022}
}


@misc{wilkins_2018_1193957,
  title = {VocalSet: A Singing Voice Dataset},
  author = {Wilkins, Julia and Seetharaman, Prem and Wahl, Alison and Pardo, Bryan},
  year = {2018},
  month = mar,
  publisher = {Zenodo},
  doi = {10.5281/zenodo.1193957}
}


@misc{chenSeniorTalkChineseConversation2025,
  title = {SeniorTalk: A Chinese Conversation Dataset with Rich Annotations for Super-Aged Seniors},
  shorttitle = {SeniorTalk},
  author = {Chen, Yang and Wang, Hui and Wang, Shiyao and Chen, Junyang and He, Jiabei and Zhou, Jiaming and Yang, Xi and Wang, Yequan and Lin, Yonghua and Qin, Yong},
  year = {2025},
  month = mar,
  number = {arXiv:2503.16578},
  eprint = {2503.16578},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.16578},
  urldate = {2025-07-05},
  abstract = {While voice technologies increasingly serve aging populations, current systems exhibit significant performance gaps due to inadequate training data capturing elderly-specific vocal characteristics like presbyphonia and dialectal variations. The limited data available on super-aged individuals in existing elderly speech datasets, coupled with overly simple recording styles and annotation dimensions, exacerbates this issue. To address the critical scarcity of speech data from individuals aged 75 and above, we introduce SeniorTalk, a carefully annotated Chinese spoken dialogue dataset. This dataset contains 55.53 hours of speech from 101 natural conversations involving 202 participants, ensuring a strategic balance across gender, region, and age. Through detailed annotation across multiple dimensions, it can support a wide range of speech tasks. We perform extensive experiments on speaker verification, speaker diarization, speech recognition, and speech editing tasks, offering crucial insights for the development of speech technologies targeting this age group.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/lichenda/Zotero/storage/U33P3RKE/Chen et al. - 2025 - SeniorTalk A Chinese Conversation Dataset with Rich Annotations for Super-Aged Seniors.pdf;/Users/lichenda/Zotero/storage/KK7D55K2/2503.html}
}


@inproceedings{zhouSeenUnseenEmotional2021,
  title = {Seen and Unseen Emotional Style Transfer for Voice Conversion with A New Emotional Speech Dataset},
  booktitle = {ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  author = {Zhou, Kun and Sisman, Berrak and Liu, Rui and Li, Haizhou},
  year = {2021},
  month = jun,
  pages = {920--924},
  issn = {2379-190X},
  doi = {10.1109/ICASSP39728.2021.9413391},
  urldate = {2025-07-05},
  keywords = {Electrostatic discharges,Emotion recognition,emotional speech dataset,emotional voice conversion,Linguistics,Signal processing,speech emotion recognition (SER),Speech recognition,Training,Transforms}
}

@article{POLQA-Beerends2013,
  title={Perceptual objective listening quality assessment ({POLQA}), the third generation {ITU-T} standard for end-to-end speech quality measurement part {I}--—temporal alignment},
  author={Beerends, John G and Schmidmer, Christian and Berger, Jens and Obermann, Matthias and Ullmann, Raphael and Pomy, Joachim and Keyhl, Michael},
  journal={Journal of The Audio Engineering Society},
  volume={61},
  number={6},
  pages={366--384},
  year={2013},
  publisher={Audio Engineering Society},
}


@inproceedings{pirklbauerEvaluationMetricsGenerative2023a,
  title = {Evaluation Metrics for Generative Speech Enhancement Methods: Issues and Perspectives},
  shorttitle = {Evaluation Metrics for Generative Speech Enhancement Methods},
  booktitle = {Speech Communication; 15th ITG Conference},
  author = {Pirklbauer, Jan and Sach, Marvin and Fluyt, Kristoff and Tirry, Wouter and Wardah, Wafaa and Moeller, Sebastian and Fingscheidt, Tim},
  year = {2023},
  month = sep,
  pages = {265--269},
  doi = {10.30420/456164052},
  urldate = {2025-07-05},
}

@misc{saekiSpeechBERTScoreReferenceAwareAutomatic2024a,
  title = {SpeechBERTScore: Reference-Aware Automatic Evaluation of Speech Generation Leveraging NLP Evaluation Metrics},
  shorttitle = {SpeechBERTScore},
  author = {Saeki, Takaaki and Maiti, Soumi and Takamichi, Shinnosuke and Watanabe, Shinji and Saruwatari, Hiroshi},
  year = {2024},
  month = sep,
  number = {arXiv:2401.16812},
  eprint = {2401.16812},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.16812},
  urldate = {2025-07-05},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/lichenda/Zotero/storage/VLMXPNDV/Saeki et al. - 2024 - SpeechBERTScore Reference-Aware Automatic Evaluation of Speech Generation Leveraging NLP Evaluation.pdf;/Users/lichenda/Zotero/storage/VYKG39RK/2401.html}
}

@article{biUse-Friedman1937,
  title={The use of ranks to avoid the assumption of normality implicit in the analysis of variance},
  author={Friedman, Milton},
  journal={Journal of the American Statistical Association},
  volume={32},
  number={200},
  pages={675--701},
  year={1937},
  publisher={Taylor \& Francis},
}

@article{universa,
  title={Uni-VERSA: Versatile Speech Assessment with a Unified Network},
  author={Shi, Jiatong and Shim, Hye-Jin and Watanabe, Shinji},
  journal={arXiv preprint arXiv:2505.20741},
  year={2025}
}

@article{universa_ext,
  title={Improving Speech Enhancement with Multi-Metric Supervision from Learned Quality Assessment},
  author={Wang, Wei and Zhang, Wangyou and Li, Chenda and Shi, Jiatong and Watanabe, Shinji and Qian, Yanmin},
  journal={arXiv preprint arXiv:2506.12260},
  year={2025}
}