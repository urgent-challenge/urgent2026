
@inproceedings{Hi_Fi-Bakhturina2021,
  title={Hi-Fi Multi-Speaker English TTS Dataset},
  author={Bakhturina, Evelina and Lavrukhin, Vitaly and Ginsburg, Boris and Zhang, Yang},
  booktitle={Proc. Interspeech},
  pages={2776--2780},
  year={2021},
  url={https://www.isca-archive.org/interspeech_2021/bakhturina21_interspeech.html},
  doi={10.21437/Interspeech.2021-1599},
}

@inproceedings{DNSMOS-Reddy2022,
  title={DNSMOS P.835: A Non-Intrusive Perceptual Objective Speech Quality Metric to Evaluate Noise Suppressors},
  author={Reddy, Chandan KA and Gopal, Vishak and Cutler, Ross},
  booktitle={Proc. ICASSP},
  pages={886--890},
  year={2022},
  url={https://arxiv.org/abs/2110.01763},
  doi={10.1109/ICASSP43922.2022.9746108},
}

@misc{liLessMoreData2025,
  title = {Less Is More: Data Curation Matters in Scaling Speech Enhancement},
  shorttitle = {Less Is {{More}}},
  author = {Li, Chenda and Zhang, Wangyou and Wang, Wei and Scheibler, Robin and Saijo, Kohei and Cornell, Samuele and Fu, Yihui and Sach, Marvin and Ni, Zhaoheng and Kumar, Anurag and Fingscheidt, Tim and Watanabe, Shinji and Qian, Yanmin},
  year = {2025},
  month = jun,
  number = {arXiv:2506.23859},
  eprint = {2506.23859},
  primaryclass = {eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.23859},
  urldate = {2025-07-01},
  abstract = {The vast majority of modern speech enhancement systems rely on data-driven neural network models. Conventionally, larger datasets are presumed to yield superior model performance, an observation empirically validated across numerous tasks in other domains. However, recent studies reveal diminishing returns when scaling speech enhancement data. We focus on a critical factor: prevalent quality issues in ``clean'' training labels within large-scale datasets. This work re-examines this phenomenon and demonstrates that, within large-scale training sets, prioritizing high-quality training data is more important than merely expanding the data volume. Experimental findings suggest that models trained on a carefully curated subset of 700 hours can outperform models trained on the 2,500-hour full dataset. This outcome highlights the crucial role of data curation in scaling speech enhancement systems effectively.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/lichenda/Zotero/storage/CSYCUI3Q/Li et al. - 2025 - Less is More Data Curation Matters in Scaling Speech Enhancement.pdf;/Users/lichenda/Zotero/storage/8H2YZQ7S/2506.html}
}
