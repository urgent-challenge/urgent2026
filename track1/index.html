<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script src="/urgent2026/assets/js/distillpub/template.v2.js"></script> <script src="/urgent2026/assets/js/distillpub/transforms.v2.js"></script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Track1: Universal Speech Enhancement | URGENT Challenge (2026) </title> <meta name="author" content="URGENT Challenge (2026)"> <meta name="description" content="Universality, Robustness, and Generalizability for EnhancemeNT"> <meta name="keywords" content="speech enhancement"> <link rel="stylesheet" href="/urgent2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/urgent2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/urgent2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/urgent2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/urgent2026/assets/img/logo.jpg?6cb554ce913190e9422d73779c840cc3"> <link rel="stylesheet" href="/urgent2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://urgent-challenge.github.io/urgent2026/track1/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <li class="nav-item dropdown" style="list-style-type: none;"> <a class="navbar-brand title font-weight-lighter dropdown-toggle" href="#" id="yearDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">URGENTÂ <span class="font-weight-bold">Challenge (2026)</span></a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="yearDropdown"> <a class="dropdown-item" href="https://urgent-challenge.github.io/urgent2024/">2024</a> <a class="dropdown-item" href="https://urgent-challenge.github.io/urgent2025/">2025</a> <a class="dropdown-item" href="/urgent2026/">2026</a> </div> </li> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/urgent2026/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/urgent2026/timeline/">Timeline </a> </li> <li class="nav-item dropdown active"> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Track1 <span class="sr-only">(current)</span> </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/urgent2026/track1#baseline">Baseline</a> <a class="dropdown-item " href="/urgent2026/track1#datasets">Data</a> <a class="dropdown-item " href="/urgent2026/track1#rules">Rules</a> <a class="dropdown-item " href="/urgent2026/track1#ranking">Ranking</a> </div> </li> <li class="nav-item "> <a class="nav-link" href="/urgent2026/track2/">Track2 </a> </li> <li class="nav-item "> <a class="nav-link" href="/urgent2026/submission/">Submission </a> </li> <li class="nav-item "> <a class="nav-link" href="/urgent2026/leaderboard/">Leaderboard </a> </li> <li class="nav-item "> <a class="nav-link" href="/urgent2026/faq/">FAQ </a> </li> <li class="nav-item "> <a class="nav-link" href="/urgent2026/notice/">Notice </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">About Us </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/urgent2026/organizers/">Organizers</a> <a class="dropdown-item " href="/urgent2026/contact/">Contact</a> </div> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Track1: Universal Speech Enhancement</h1> <p class="post-description"></p> </header> <article> <h2 id="contents">Contents:</h2> <ul> <li><a href="#contents">Contents:</a></li> <li><a href="#baseline">Baseline</a></li> <li> <a href="#datasets">Datasets</a> <ul> <li><a href="#brief-data-description">Brief data description:</a></li> <li><a href="#detailed-data-description">Detailed data description:</a></li> </ul> </li> <li><a href="#rules">Rules</a></li> <li> <a href="#ranking">Ranking</a> <ul> <li><a href="#overall-ranking-method">Overall ranking method</a></li> </ul> </li> </ul> <h2 id="baseline">Baseline</h2> <p>Please refer to the official <a href="https://github.com/urgent-challenge/urgent2026_challenge_track1" rel="external nofollow noopener" target="_blank">GitHub repository</a> for more details.</p> <p><strong>Discriminative Baseline.</strong> We provide an adaptive STFT-based SFI<d-cite key="Sampling-Paulus2022,Toward-Zhang2023,Improving-Zhang2024"></d-cite> BSRNN <d-cite key="Music-Luo2023,yuEfficientMonauralSpeech2023,High-Yu2023"></d-cite> as the discriminative baseline. The model is available at <a href="https://github.com/urgent-challenge/urgent2026_challenge_track1" rel="external nofollow noopener" target="_blank">here</a>. The details of the baseline training can be found in our <a href="https://arxiv.org/abs/2506.23859" rel="external nofollow noopener" target="_blank">recent ASRU paper</a> <d-cite key="liLessMoreData2025"></d-cite>.</p> <p><strong>Generative Baseline.</strong> We follow a recent work named FlowSE<d-cite key="leeFlowSEFlowMatchingbased2025"></d-cite> to build generative SE models. It extends the flow matching method<d-cite key="lipmanFlowMatchingGenerative2023"></d-cite> to a conditional flow matching model that generates clean speech conditioned by the noisy speech. We reimplement an <a href="https://github.com/urgent-challenge/urgent2026_challenge_track1/blob/main/baseline_code/models/bsrnn_flowse.py" rel="external nofollow noopener" target="_blank">improved BSRNN</a> to estimate the conditional vector field. The model is available at <a href="https://github.com/urgent-challenge/urgent2026_challenge_track1" rel="external nofollow noopener" target="_blank">here</a>, and the training details of the generative baseline model can be found in the <a href="https://arxiv.org/abs/2506.23859" rel="external nofollow noopener" target="_blank">paper</a> <d-cite key="liLessMoreData2025"></d-cite>.</p> <p><br></p> <h2 id="datasets">Datasets</h2> <h3 id="brief-data-description">Brief data description:</h3> <p>The training and validation data are both simulated by using several public speech/noise/rir corpora (see the table below for more details). We provide the <a href="https://github.com/urgent-challenge/urgent2026_challenge_track1/blob/main/utils/prepare_train_data.sh" rel="external nofollow noopener" target="_blank">data preparation pipeline</a> with the <a href="https://github.com/urgent-challenge/urgent2026_challenge_track1" rel="external nofollow noopener" target="_blank">official baseline</a>, which automatically downloads and pre-processes the data.</p> <p>Non-blind/bind test set will be released later, check the <a href="/urgent2026/timeline/">timeline</a> page for more details.</p> <h3 id="detailed-data-description">Detailed data description:</h3> <p>Based on the <a href="https://urgent-challenge.github.io/urgent2025/data/">dataset of the 2nd URGENT challenge</a>, we conducted a data selection using the data filtering method proposed in a recent paper <d-cite key="liLessMoreData2025"></d-cite>.</p> <p><strong>It is noted that we encourage you to explore better ways of data selection and utilization in this challenge.</strong> In addition to the data and filtering methods provided by our baseline, you can make use of larger-scale datasets, such as the <a href="https://urgent-challenge.github.io/urgent2025/data/">track2 data</a> from the 2nd URGENT challenge, or other allowed data (please check it in the <a href="/urgent2026/rules/">rules</a> section).</p> <p>The training and validation data are both simulated based on the following source data.</p> <style>table{border-spacing:0;border-collapse:collapse;width:100%;max-width:100%;margin-bottom:15px;background-color:transparent;text-align:left}th{font-weight:bold;border:1px solid #ccc;padding:8px}td{border:1px solid #ccc;padding:8px}tr{background-color:white}tr:nth-of-type(2n){background-color:#f6f8fa}.light-keys tr:nth-of-type(2n){background-color:black}.light-keys tr:hover{background-color:black}.light-keys table{border:0}.light-keys tr{border:0}.light-keys td{border:0}.light-keys th{border:0}tr th{background-color:white}tr{-moz-transition:background-color 300ms ease-in-out 0;-ms-transition:background-color 300ms ease-in-out 0;-o-transition:background-color 300ms ease-in-out 0;-webkit-transition:background-color 300ms ease-in-out 0;transition:background-color 300ms ease-in-out 0}tr:hover{background-color:#fff176}tr{border-top:1px solid #ccc;border-bottom:1px solid #ccc}</style> <table> <colgroup> <col> <col> <col> <col> <col> <col> </colgroup> <thead> <tr> <th>Type</th> <th>Corpus</th> <th>Condition</th> <th>Sampling Frequency (kHz)</th> <th>Duration of in 2nd URGENT</th> <th>Duration of in 3rd URGENT</th> <th>License</th> </tr> </thead> <tbody> <tr> <td rowspan="10">Speech</td> <td>LibriVox data from <a href="https://github.com/microsoft/DNS-Challenge/blob/master/download-dns-challenge-5-headset-training.sh" rel="external nofollow noopener" target="_blank">DNS5 challenge</a> </td> <td>Audiobook</td> <td>8~48</td> <td>~350 h</td> <td>~150 h</td> <td>CC BY 4.0</td> </tr> <tr> <td>LibriTTS reading speech</td> <td>Audiobook</td> <td>8~24</td> <td>~200 h</td> <td>~109 h</td> <td>CC BY 4.0</td> </tr> <tr> <td>VCTK reading speech</td> <td>Newspaper, etc.</td> <td>48</td> <td>~80 h</td> <td>~44 h</td> <td>ODC-BY</td> </tr> <tr> <td>EARS speech</td> <td>Studio recording</td> <td>48</td> <td>~100 h</td> <td>~16 h</td> <td> CC-NC 4.0</td> </tr> <tr> <td>Multilingual Librispeech (de, en, es, fr)<d-footnote>We collected less compressed MLS from LibriVox, which have higher audio quality than the original MLS for ASR.</d-footnote> </td> <td>Audiobook</td> <td>8~48</td> <td>~450 (48600) h</td> <td>~129 h</td> <td>CC0</td> </tr> <tr> <td>CommonVoice 19.0 (de, en, es, fr, zh-CN)</td> <td>Crowd-sourced voices</td> <td>8~48</td> <td>~1300 (9500) h</td> <td>~250 h</td> <td>CC0</td> </tr> <tr> <td>NNCES</td> <td>Children speech</td> <td>44.1</td> <td>-</td> <td>~20 h</td> <td>CC0</td> </tr> <tr> <td>SeniorTalk</td> <td>Elderly speech</td> <td>16</td> <td>-</td> <td>~50 h</td> <td>CC BY-NC-SA 4.0</td> </tr> <tr> <td>VocalSet</td> <td>Singing voice</td> <td>44.1</td> <td>-</td> <td>~10 h</td> <td>CC BY 4.0</td> </tr> <tr> <td>ESD</td> <td>Emotional speech</td> <td>16</td> <td>-</td> <td>~30 h</td> <td>non-commercial, <a href="https://github.com/HLTSingapore/Emotional-Speech-Data" rel="external nofollow noopener" target="_blank">custom</a> <d-footnote>You need to sign a license to obtain this dataset.</d-footnote> </td> </tr> </tbody> </table> <p>For the noise source and RIRs, we follow the same configuration as in the <a href="https://urgent-challenge.github.io/urgent2025/data/">2nd URGENT challenge</a>.</p> <table> <colgroup> <col> <col> <col> <col> <col> <col> </colgroup> <thead> <tr> <th>Type</th> <th>Corpus</th> <th>Condition</th> <th>Sampling Frequency (kHz)</th> <th>Duration of in 2nd URGENT</th> <th>License</th> </tr> </thead> <tbody> <tr> <td rowspan="5">Noise</td> <td>Audioset+FreeSound noise in DNS5 challenge</td> <td>Crowd-sourced + Youtube</td> <td>8~48</td> <td>~180 h</td> <td>CC BY 4.0</td> </tr> <tr> <td>WHAM! noise</td> <td>4 Urban environments</td> <td>48</td> <td>~70 h</td> <td>CC BY-NC 4.0</td> </tr> <tr> <td>FSD50K (human voice filtered)</td> <td>Crowd-sourced</td> <td>8~48</td> <td>~100 h</td> <td>CC0, CC-BY, CC-BY-NC, CC Sampling+</td> </tr> <tr> <td>Free Music Archive (medium)</td> <td>Free Music Archive (directed by WFMU) </td> <td>8~44.1</td> <td>~200 h</td> <td>CC</td> </tr> <tr> <td>Wind noise simulated by participants</td> <td>-</td> <td>any</td> <td>-</td> <td>N/A</td> </tr> <tr> <td rowspan="2">RIR</td> <td>Simulated RIRs from DNS5 challenge</td> <td><a href="https://www.openslr.org/28/" rel="external nofollow noopener" target="_blank">SLR28</a></td> <td>48</td> <td>~60k samples</td> <td>CC BY 4.0</td> </tr> <tr> <td>RIRs simulated by participants</td> <td>-</td> <td>any</td> <td>-</td> <td>N/A</td> </tr> </tbody> </table> <p>Note that, except for the data listed above and those we allow in the <a href="#rules">rules</a>, they cannot be used for the purpose of this challenge. However, we allow participants to simulate their own RIRs using existing tools for generating the training data. The participants can also propose publicly available, real recorded RIRs to be included in the above data list during the grace period. See <a href="#rules">rules</a> section for more details.</p> <p><strong>Data selection and Simulation.</strong> We apply the data selection to the track1 data of the <a href="https://urgent-challenge.github.io/urgent2025/">2nd URGENT</a> using the data filtering method proposed in the recent paper <d-cite key="liLessMoreData2025"></d-cite>. The selected data list is available at <a href="https://github.com/urgent-challenge/urgent2026_challenge_track1/blob/main/meta/train_selected_700h" rel="external nofollow noopener" target="_blank">here</a>. The speech source from NNCES, SeniorTalk, VocalSet, and ESD is not filtered.</p> <p>Note that the data filtering of paper <d-cite key="liLessMoreData2025"></d-cite> is obviously not the best method to utilize the large-scale dataset for SE. <strong>The goal of this challenge is to encourage participants to develop how to better leverage large-scale data</strong> to improve the SE performance.</p> <p>The simulation data can be generated as follows:</p> <ol> <li> <p>In the first step, a manifest <code class="language-plaintext highlighter-rouge">meta.tsv</code> is first generated by <a href="https://github.com/urgent-challenge/urgent2025_challenge/blob/main/simulation/generate_data_param.py" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">simulation/generate_data_param.py</code></a> from the given list of speech, noise, and room impulse response (RIR) samples. It specifies how each sample will be simulated, including the type of distortion to be applied, the speech/noise/RIR sample to be used, the signal-to-noise ratio (SNR), the random seed, and so on.</p> </li> <li> <p>In the second step, the simulation can be done in parallel via <a href="https://github.com/urgent-challenge/urgent2025_challenge/blob/main/simulation/simulate_data_from_param.py" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">simulation/simulate_data_from_param.py</code></a> for different samples according to the manifest while ensuring reproducibility. This procedure can be used to generate training and validation datasets.</p> </li> <li> <p>By default, we applied a <a href="https://github.com/urgent-challenge/urgent2026_challenge_track1/blob/main/simulation/simulate_data_from_param.py" rel="external nofollow noopener" target="_blank">high-pass filter</a> to the speech signals since we have noticed that there is high-energy noise in the infrasound frequency band in some speech sources. You can turn it off by setting <code class="language-plaintext highlighter-rouge">highpass=False</code> in your simulation.</p> </li> </ol> <p>A pre-simulated training and validation dataset<d-footnote>The simulated speech derived from the ESD dataset is not included due to the license issue, you may apply for the license and run the simulation script by yourself to obtain it.<d-footnote> is available online at [HugginFace](https://huggingface.co/datasets/lichenda/urgent26_track2_universal_se). Participants can download and use it directly without running the simulation. For the training set, **we recommend dynamically generating degraded speech samples during training** to increase the data diversity.</d-footnote></d-footnote></p> <p><img alt="framework" src="/urgent2026/assets/img/framework.jpg" style="max-width: 100%;"></p> <p><strong>Distortions Model.</strong> As depicted in the figure above, we design a distortion model (simulation stage) <d-math>\mathcal{F}(\cdot)</d-math> to unify the data format for different distortion types, such that different speech enhancement (SE) sub-tasks can share a consistent input/output processing. In particular, we ensure that the sampling frequency (SF) at the output of the distortion model (degraded speech) is always equal to that of its input.</p> <p>During training and inference, the processing of different SFs is supported for both <u>conventional SE models</u> (lower-right) that usually only operate at one SF and <u>adaptive STFT-based sampling-frequency-independent (SFI) SE models</u> (upper-right) that can directly handle different SFs.</p> <ul> <li>For conventional SE models (e.g., Conv-TasNet<d-cite key="Conv_TasNet-Luo2019"></d-cite>), we always <strong>upsample</strong> its input (degraded speech) to the highest SF (48 kHz) so that the model only needs to operate at 48 kHz. The model output (48 kHz) is then <strong>downsampled</strong> to the same SF as the degraded speech.</li> <li>For adaptive STFT-based SFI<d-cite key="Sampling-Paulus2022,Toward-Zhang2023,Improving-Zhang2024"></d-cite> SE models (e.g., BSRNN<d-cite key="Music-Luo2023,yuEfficientMonauralSpeech2023,High-Yu2023"></d-cite>, TF-GridNet<d-cite key="TF_GridNet-Wang2023,TF_GridNet2-Wang2023"></d-cite>), we directly feed the degraded speech of different SFs into the model, which can adaptively adjust their STFT/iSTFT configuration according to the SF and generate the enhanced signal with the same SF. <br> </li> </ul> <p>In the challenge, the SE system has to address the following seven distortions:</p> <ol> <li>Additive noise</li> <li>Reverberation</li> <li>Clipping</li> <li>Bandwidth limitation</li> <li>Codec distortion</li> <li>Packet loss</li> <li>Wind noise</li> </ol> <p>We provide an example simulation script as <a href="https://github.com/urgent-challenge/urgent2026_challenge_track1/blob/main/simulation/simulate_data_from_param.py" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">simulation/simulate_data_from_param.py</code></a>.</p> <p><br></p> <h2 id="rules">Rules</h2> <ol> <li> <p>When generating the training and validation datasets, <strong>ONLY the speech, noise, and room impulse response (RIR) corpora listed in the <a href="/urgent2025/data"><code class="language-plaintext highlighter-rouge">Data</code></a> tab shall be used to ensure a fair comparison</strong> and proper understanding of various SE approaches.</p> <ul> <li> <p>The first month of the challenge will be a grace period when participants can propose additional public datasets to be included in the list. We (organizers) will reply to the requests and may update the list. Updates will be recorded in the <a href="/urgent2026/notice"><code class="language-plaintext highlighter-rouge">Notice</code></a> tab.</p> </li> <li> <strong>It is NOT allowed to use pre-trained speech enhancement models trained on other than official Challenge data</strong>. <ul> <li>However, it IS allowed to use a pre-trained model trained on the official challenge data (e.g., <a href="https://huggingface.co/wyz/tfgridnet_for_urgent24" rel="external nofollow noopener" target="_blank">URGENT 2024</a> / <a href="https://huggingface.co/kohei0209/tfgridnet_urgent25" rel="external nofollow noopener" target="_blank">URGENT 2025</a> / <a href="TODO%20link">URGENT 2026</a> official baseline )</li> </ul> </li> <li>Although the speech enhancement model should only be trained on the listed data, we allow the use of pre-trained foundation models such as <a href="https://github.com/facebookresearch/fairseq/blob/main/examples/hubert/README.md" rel="external nofollow noopener" target="_blank">HuBERT</a>, <a href="https://github.com/microsoft/unilm/blob/master/wavlm/README.md" rel="external nofollow noopener" target="_blank">WavLM</a>, <a href="https://github.com/facebookresearch/encodec" rel="external nofollow noopener" target="_blank">EnCodec</a>, <a href="https://llama.meta.com/llama-downloads/" rel="external nofollow noopener" target="_blank">Llama</a>, and so on. <strong>We also allow the use of a pretrained speech enhancement/restoration model to improve the quality of clean speech for simulation.</strong> The use of all pre-trained models must meet the following requirements: <ul> <li>They are publicly available before the challenge begins</li> <li>They are explicitly mentioned in the submitted system description.</li> <li>Note: <ul> <li>Their parameters can be fine-tuned on the listed data.</li> <li>It is not allowed to fine-tune any model, be it pre-trained or not, on any extra data other than the listed data.<br><br> </li> </ul> </li> </ul> </li> <li><strong>If you are unsure whether the pre-trained model you would like to use is allowed, please reach out to the organizers.</strong></li> </ul> </li> <li>We allow participants to simulate their own RIRs using existing tools<d-footnote>For example, <a href="https://github.com/ehabets/RIR-Generator" rel="external nofollow noopener" target="_blank">RIR-Generator</a>, <a href="https://github.com/LCAV/pyroomacoustics" rel="external nofollow noopener" target="_blank">pyroomacoustics</a>, <a href="https://github.com/DavidDiazGuerra/gpuRIR" rel="external nofollow noopener" target="_blank">gpuRIR</a>, and so on.</d-footnote> for generating the training data. The participants can also propose publicly available, real recorded RIRs to be included in the above data list during the grace period (See <a href="/urgent2026/timeline"><code class="language-plaintext highlighter-rouge">Timeline</code></a>). <blockquote> <p>Note: If participants used additional RIRs to train their model, the related information should be provided in the README.yaml file in the submission. Check the <a href="/urgent2026/template">template</a> for more information.</p> </blockquote> </li> <li> <p>We allow participants to simulate wind noise using some tools such as <a href="https://github.com/audiolabs/SC-Wind-Noise-Generator/tree/main" rel="external nofollow noopener" target="_blank">SC-Wind-Noise-Generator</a>. In default, the simulation script in our repository simulates 200 and 100 wind noises for training and validation for each sampling frequency. The configuration can be easily changed in <a href="https://github.com/urgent-challenge/urgent2026_challenge_track1/blob/main/conf/wind_noise_simulation_train.yaml" rel="external nofollow noopener" target="_blank">wind_noise_simulation_train.yaml</a> and <a href="https://github.com/urgent-challenge/urgent2026_challenge_track1/blob/main/conf/wind_noise_simulation_validation.yaml" rel="external nofollow noopener" target="_blank">wind_noise_simulation_validation.yaml</a></p> </li> <li> <p>The test data should only be used for evaluation purposes. <strong>Techniques such as test-time adaptation, unsupervised domain adaptation, and self-training on the test data are NOT allowed</strong>.</p> </li> <li> <p>There is no constraint on the latency or causality of the developed system in this challenge. Any type of model can be used as long as it conforms to the other rules as listed on this page.</p> </li> <li>Registration is required to submit results to the challenge (Check the <a href="/urgent2025/leaderboard"><code class="language-plaintext highlighter-rouge">Leaderboard</code></a> tab for more information). Note that the team information (including affiliation, team name, and team members) should be provided when submitting the results. For detailed submission requirements, please check the <a href="/urgent2025/submission"><code class="language-plaintext highlighter-rouge">Submission</code></a> tab. <ul> <li>Only the team name will be shown in the leaderboard, while the affiliation and team members will be kept confidential.<br><br> </li> </ul> </li> </ol> <h2 id="ranking">Ranking</h2> <p>The ranking will be carried out in <strong>two stages</strong>. In <code class="language-plaintext highlighter-rouge">stage 1</code>, we will evaluate participantsâ submissions with <strong>multiple objective metrics</strong>. The top-6 systems in the objective <code class="language-plaintext highlighter-rouge">overall ranking</code> will advance to the <code class="language-plaintext highlighter-rouge">stage 2</code>, be evaluated by <strong>multiple subjective tests</strong>, and then the <code class="language-plaintext highlighter-rouge">final ranking</code> will be determined by the subjective <code class="language-plaintext highlighter-rouge">overall ranking</code>.</p> <ol> <li> <p>The following objective evaluation metrics will be calculated for evaluation in <code class="language-plaintext highlighter-rouge">stage 1</code>. For real recorded test samples that do not have a strictly matched reference signal, part of the following metrics will be used. The <code class="language-plaintext highlighter-rouge">overall ranking</code> will be determined by the algorithm introduced in the <a href="#overall-ranking-method">subsequent section</a>.</p> <style type="text/css">.tg{border:0;border-collapse:collapse;border-color:#ccc;border-spacing:0}.tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:0;color:#333;font-family:Arial,sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal}.tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:0;color:#333;font-family:Arial,sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal}.tg .tg-r2ra{background-color:#f9f9f9;border-color:inherit;text-align:left;vertical-align:middle}.tg .tg-51oy{background-color:#fff;border-color:#000;text-align:center;vertical-align:middle}.tg .tg-rt8k{background-color:#fff;border-color:#000;text-align:left;vertical-align:middle}.tg .tg-uzvj{border-color:inherit;font-weight:bold;text-align:center;vertical-align:middle}.tg .tg-g7sd{border-color:inherit;font-weight:bold;text-align:left;vertical-align:middle}.tg .tg-r6l2{background-color:#fff;border-color:inherit;text-align:center;vertical-align:middle}.tg .tg-0a7q{border-color:#000;text-align:left;vertical-align:middle}.tg .tg-xwyw{border-color:#000;text-align:center;vertical-align:middle}.tg .tg-kyy7{background-color:#f9f9f9;border-color:inherit;text-align:center;vertical-align:middle}.tg .tg-d459{background-color:#f9f9f9;border-color:inherit;text-align:left;vertical-align:middle}.tg .tg-ligs{background-color:#f9f9f9;border-color:inherit;text-align:center;vertical-align:middle}.tg .tg-rq3n{background-color:#fff;border-color:inherit;text-align:center;vertical-align:middle}.tg .tg-mfxt{background-color:#fff;border-color:inherit;text-align:left;vertical-align:middle}.tg .tg-qmuc{background-color:#fff;border-color:inherit;text-align:left;vertical-align:middle}</style> <table class="tg"> <thead> <tr> <th class="tg-uzvj">Category</th> <th class="tg-g7sd">Metric</th> <th class="tg-uzvj">Need Reference Signals?</th> </tr> </thead> <tbody> <tr> <td class="tg-r6l2" rowspan="4">Non-intrusive SE metrics</td> <td class="tg-rt8k"> <a href="https://github.com/urgent-challenge/urgent2025_challenge/blob/main/evaluation_metrics/calculate_nonintrusive_dnsmos.py" rel="external nofollow noopener" target="_blank">DNSMOS</a> â<d-cite key="DNSMOS-Reddy2022"></d-cite> </td> <td class="tg-51oy">â</td> </tr> <tr> <td class="tg-0a7q"> <a href="https://github.com/urgent-challenge/urgent2025_challenge/blob/main/evaluation_metrics/calculate_nonintrusive_nisqa.py" rel="external nofollow noopener" target="_blank">NISQA</a> â<d-cite key="NISQA-Mittag2021"></d-cite> </td> <td class="tg-xwyw"><span style="font-weight:400;font-style:normal;text-decoration:none">â</span></td> </tr> <tr> <td class="tg-0a7q"> <a href="https://github.com/urgent-challenge/urgent2025_challenge/blob/main/evaluation_metrics/calculate_nonintrusive_mos.py" rel="external nofollow noopener" target="_blank">UTMOS</a> â<d-cite key="UTMOS-SAEKI2022"></d-cite> </td> <td class="tg-xwyw"><span style="font-weight:400;font-style:normal;text-decoration:none">â</span></td> </tr> <tr> <td class="tg-0a7q"> <a href="https://github.com/alessandroragano/scoreq" rel="external nofollow noopener" target="_blank">ScoreQ</a> â<d-cite key="NEURIPS2024_bece7e02"></d-cite> </td> <td class="tg-xwyw"><span style="font-weight:400;font-style:normal;text-decoration:none">â</span></td> </tr> <tr> <td class="tg-kyy7" rowspan="3">Intrusive SE metrics</td> <td class="tg-d459"> <a href="http://www.polqa.info" style="color:#e97c36;" rel="external nofollow noopener" target="_blank">POLQA</a><d-footnote>This metric will only be used for evaluation of the final blind test set.</d-footnote> â</td> <td class="tg-kyy7">â</td> </tr> <tr> <td class="tg-d459"> <a href="https://github.com/urgent-challenge/urgent2025_challenge/blob/main/evaluation_metrics/calculate_intrusive_se_metrics.py" rel="external nofollow noopener" target="_blank">PESQ</a> â<d-cite key="PESQ-Rix2001"></d-cite> </td> <td class="tg-kyy7">â</td> </tr> <tr> <td class="tg-r2ra"> <a href="https://github.com/urgent-challenge/urgent2025_challenge/blob/main/evaluation_metrics/calculate_intrusive_se_metrics.py" rel="external nofollow noopener" target="_blank">ESTOI</a> â<d-cite key="ESTOI-Jensen2016"></d-cite> </td> <td class="tg-ligs">â</td> </tr> <tr> <td class="tg-rq3n" rowspan="2">Downstream-task-independent metrics</td> <td nowrap class="tg-mfxt"> <a href="https://github.com/urgent-challenge/urgent2025_challenge/blob/main/evaluation_metrics/calculate_speechbert_score.py" rel="external nofollow noopener" target="_blank">SpeechBERTScore</a><d-footnote>To evaluate multilingual speech, we adopt the MHuBERT-147 backend for calculating the SpeechBERTScore, which differs from its default backend (WavLM-Large).</d-footnote> â<d-cite key="SpeechBERTScore-Saeki2024"></d-cite> </td> <td class="tg-rq3n">â</td> </tr> <tr> <td class="tg-qmuc"> <a href="https://github.com/urgent-challenge/urgent2025_challenge/blob/main/evaluation_metrics/calculate_phoneme_similarity.py" rel="external nofollow noopener" target="_blank">LPS</a> â<d-cite key="Evaluation-Pirklbauer2023"></d-cite> </td> <td class="tg-r6l2">â</td> </tr> <tr> <td class="tg-ligs" rowspan="4">Downstream-task-dependent metrics</td> <td class="tg-r2ra"> <a href="https://github.com/urgent-challenge/urgent2025_challenge/blob/main/evaluation_metrics/calculate_speaker_similarity.py" rel="external nofollow noopener" target="_blank">Speaker Similarity</a> â</td> <td class="tg-ligs">â</td> </tr> <tr> <td class="tg-d459"> <a href="https://github.com/wavlab-speech/versa/blob/main/versa/utterance_metrics/emotion.py" rel="external nofollow noopener" target="_blank">Emotion Similarity</a> â</td> <td class="tg-kyy7">â</td> </tr> <tr> <td class="tg-d459"> <a href="https://github.com/wavlab-speech/versa/blob/d8ffb153e52ddfd7edbff90c697d52c359e62c5a/versa/utterance_metrics/owsm_lid.py" rel="external nofollow noopener" target="_blank">Language identification accuracy</a> â</td> <td class="tg-kyy7">â</td> </tr> <tr> <td class="tg-d459"> <a href="https://github.com/urgent-challenge/urgent2025_challenge/blob/main/evaluation_metrics/calculate_wer.py" rel="external nofollow noopener" target="_blank">Character accuracy (1 - CER)</a> â</td> <td class="tg-kyy7">â</td> </tr> </tbody> </table> <p><br></p> </li> <li> <p>In <code class="language-plaintext highlighter-rouge">stage 2</code>, we will make <a href="https://www.itu.int/rec/T-REC-P.808-202106-I" rel="external nofollow noopener" target="_blank">ITU-T P.808</a> Absolute Category Rating (ACR) and Comparison Category Rating (CCR) for the top-6 submissions, and the Mean Opinion Score (MOS) and Comparison Mean Opinion Score (CMOS) will be used for subjective metric ranking. Then the <code class="language-plaintext highlighter-rouge">overall ranking</code> in the subjective evaluation will become the <code class="language-plaintext highlighter-rouge">final ranking</code> of the challenge. It is noted that, if the <code class="language-plaintext highlighter-rouge">overall rankings</code> of the subjective tests of two teams are the same, we will refer to the <code class="language-plaintext highlighter-rouge">overall rankings</code> of the objective tests to determine the <code class="language-plaintext highlighter-rouge">final rankings</code>.</p> <table class="tg"> <thead> <tr> <th class="tg-uzvj">Category</th> <th class="tg-g7sd">Metric</th> <th class="tg-uzvj">Evaluation method</th> </tr> </thead> <tbody> <tr> <td class="tg-r6l2" rowspan="2">Subjective metrics</td> <td class="tg-rt8k">MOS â</td> <td class="tg-51oy"> <a href="https://www.itu.int/rec/T-REC-P.808-202106-I" rel="external nofollow noopener" target="_blank">ITU-T P.808</a> ACR</td> </tr> <tr> <td class="tg-0a7q">CMOS â</td> <td class="tg-51oy"> <a href="https://www.itu.int/rec/T-REC-P.808-202106-I" rel="external nofollow noopener" target="_blank">ITU-T P.808</a> CCR</td> </tr> </tbody> </table> <p><br></p> </li> </ol> <h3 id="overall-ranking-method">Overall ranking method</h3> <p>The overall ranking will be determined via the following procedure:</p> <ol> <li>Calculate the <code class="language-plaintext highlighter-rouge">average score</code> of each metric for each submission.</li> <li>Calculate the <code class="language-plaintext highlighter-rouge">per-metric ranking</code> based on the average score. <ul> <li>We adopt the dense ranking (â1223â ranking)<d-footnote><a href="https://en.wikipedia.org/wiki/Ranking#Dense_ranking_(%221223%22_ranking)" rel="external nofollow noopener" target="_blank">https://en.wikipedia.org/wiki/Ranking#Dense_ranking_("1223"_ranking)</a></d-footnote> strategy for handling ties.</li> </ul> </li> <li>Calculate the <code class="language-plaintext highlighter-rouge">per-category ranking</code> by averaging the rankings within each category.</li> <li>Calculate the <code class="language-plaintext highlighter-rouge">overall ranking</code> by averaging the <code class="language-plaintext highlighter-rouge">per-category rankings</code>.<br><br> </li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Step 1: Calculate the average score of each metric
</span><span class="n">scores</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">submission</span> <span class="ow">in</span> <span class="n">all_submissions</span><span class="p">:</span>
  <span class="n">scores</span><span class="p">[</span><span class="n">submission</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="k">for</span> <span class="n">category</span> <span class="ow">in</span> <span class="n">metric_categories</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="n">category</span><span class="p">:</span>
      <span class="n">scores</span><span class="p">[</span><span class="n">submission</span><span class="p">][</span><span class="n">metric</span><span class="p">]</span> <span class="o">=</span> <span class="nf">mean</span><span class="p">([</span><span class="nf">metric</span><span class="p">(</span><span class="n">each_sample</span><span class="p">)</span> <span class="k">for</span> <span class="n">each_sample</span> <span class="ow">in</span> <span class="n">submission</span><span class="p">])</span>

<span class="c1"># Step 2: Calculate the per-metric ranking based on the average score
</span><span class="n">rank_per_metric</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">rank_per_category</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">category</span> <span class="ow">in</span> <span class="n">metric_categories</span><span class="p">:</span>
  <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="n">category</span><span class="p">:</span>
    <span class="n">rank_per_metric</span><span class="p">[</span><span class="n">metric</span><span class="p">]</span> <span class="o">=</span> <span class="nf">get_ranking</span><span class="p">([</span><span class="n">scores</span><span class="p">[</span><span class="n">submission</span><span class="p">][</span><span class="n">metric</span><span class="p">]</span> <span class="k">for</span> <span class="n">submission</span> <span class="ow">in</span> <span class="n">all_submissions</span><span class="p">])</span>

  <span class="c1"># Step 3: Calculate the `per-category ranking` by averaging the rankings within each category
</span>  <span class="n">rank_per_category</span><span class="p">[</span><span class="n">category</span><span class="p">]</span> <span class="o">=</span> <span class="nf">get_ranking</span><span class="p">([</span><span class="n">rank_per_metric</span><span class="p">[</span><span class="n">metric</span><span class="p">]</span> <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="n">category</span><span class="p">])</span>

<span class="c1"># Step 4: Calculate the overall ranking by averaging the `per-category rankings`
</span><span class="n">rank_overall</span> <span class="o">=</span> <span class="nf">get_ranking</span><span class="p">([</span><span class="n">rank_per_category</span><span class="p">[</span><span class="n">category</span><span class="p">]</span> <span class="k">for</span> <span class="n">category</span> <span class="ow">in</span> <span class="n">metric_categories</span><span class="p">])</span>
</code></pre></div></div> </article> </div> </div> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix><d-bibliography src="/urgent2026/assets/bibliography/track1.bib"></d-bibliography> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2025 URGENT Challenge (2026). Last updated: August 29, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/urgent2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/urgent2026/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/urgent2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/urgent2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/urgent2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/urgent2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/urgent2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/urgent2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/urgent2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/urgent2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>